<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>file_1692895468461</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .h3, h3 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s1 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s2 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s3 { color: #7F7F7F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 20pt; }
 h2 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s4 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h4 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .p, p { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; margin:0pt; }
 .s5 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .a, a { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s6 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s7 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s8 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s9 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s10 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s11 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s12 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s13 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s14 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 8pt; }
 .s15 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -3pt; }
 .s18 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 2pt; }
 .s19 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s21 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s22 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s23 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s24 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 1pt; }
 .s25 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s26 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -4pt; }
 .s27 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s28 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s29 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -10pt; }
 .s30 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s32 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -12pt; }
 .s33 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s34 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 2pt; }
 .s35 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 6pt; }
 .s36 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s37 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 5pt; vertical-align: -1pt; }
 .s38 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 15pt; }
 .s40 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s41 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s42 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s43 { color: black; font-family:"Verdana Pro Light", sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s44 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s45 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 10pt; }
 .s46 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: -1pt; }
 .s47 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 9pt; }
 .s48 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 9pt; }
 .s49 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -7pt; }
 .s50 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s51 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s52 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s53 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -7pt; }
 .s54 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -3pt; }
 .s55 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; vertical-align: 7pt; }
 .s57 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 7pt; }
 .s58 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -6pt; }
 .s59 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s60 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 3pt; }
 .s61 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s62 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s63 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s64 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s65 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s66 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s67 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 7pt; }
 .s68 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 4pt; }
 .s69 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s70 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -3pt; }
 .s72 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt; vertical-align: 5pt; }
 .s73 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; vertical-align: 7pt; }
 .s74 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 7pt; }
 .s75 { color: black; font-family:Verdana, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s76 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 4pt; }
 .s77 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s78 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: -3pt; }
 .s79 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 9pt; }
 .s80 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 6pt; }
 .s81 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: -2pt; }
 .s82 { color: black; font-family:"Comic Sans MS"; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s83 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8pt; }
 .s84 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8pt; vertical-align: -3pt; }
 .s85 { color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 8pt; }
 .s86 { color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; vertical-align: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: d1 1; }
 #l3> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l4 {padding-left: 0pt;counter-reset: c3 1; }
 #l4> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l5 {padding-left: 0pt;counter-reset: d1 4; }
 #l5> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l6 {padding-left: 0pt;counter-reset: c2 1; }
 #l6> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l6> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l7 {padding-left: 0pt;counter-reset: c3 1; }
 #l7> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l7> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l8 {padding-left: 0pt;counter-reset: d1 5; }
 #l8> li>*:first-child:before {counter-increment: d1; content: counter(d1, decimal)". "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l8> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l9 {padding-left: 0pt;counter-reset: c3 1; }
 #l9> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l9> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l10 {padding-left: 0pt;counter-reset: c3 1; }
 #l10> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l11 {padding-left: 0pt;counter-reset: c2 1; }
 #l11> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l11> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l12 {padding-left: 0pt;counter-reset: c2 1; }
 #l12> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l13 {padding-left: 0pt;counter-reset: c3 1; }
 #l13> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l13> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l14 {padding-left: 0pt;counter-reset: c3 1; }
 #l14> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: c3 0;  }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 2pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">Resource-Efficient Neural Networks for Embedded Systems</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 13pt;text-align: center;"><a href="mailto:roth@tugraz.at" style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt;" target="_blank">Wolfgang Roth                                </a><a href="mailto:roth@tugraz.at" class="s1" target="_blank">roth@tugraz.at</a></p><p class="s2" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Graz University of Technology, Austria</p><p class="s3" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">arXiv:2001.03048v2 [stat.ML] 9 Dec 2022</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Laboratory of Signal Processing and Speech Communication</p><h3 style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Gu¨nther Schindler</h3><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Heidelberg University, Germany Institute of Computer Engineering</p><h3 style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Bernhard Klein</h3><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Heidelberg University, Germany Institute of Computer Engineering</p><p style="padding-top: 5pt;text-indent: 0pt;text-align: right;"><a href="mailto:guenther.schindler@ziti.uni-heidelberg.de" class="s1">guenther.schindler@ziti.uni-heidelberg.de</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: right;"><a href="mailto:bernhard.klein@ziti.uni-heidelberg.de" class="s1">bernhard.klein@ziti.uni-heidelberg.de</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="mailto:robert.peharz@tugraz.at" style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt;" target="_blank">Robert Peharz                           </a><a href="mailto:robert.peharz@tugraz.at" class="s1" target="_blank">robert.peharz@tugraz.at</a></p><p class="s2" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Graz University of Technology, Austria Institute of Theoretical Computer Science</p><h3 style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Sebastian Tschiatschek <span class="s2">University of Vienna, Austria Faculty of Computer Science</span></h3><h3 style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Holger Fr¨oning</h3><p class="s2" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Heidelberg University, Germany Institute of Computer Engineering</p><p style="padding-top: 4pt;text-indent: 0pt;text-align: right;"><a href="mailto:sebastian.tschiatschek@univie.ac.at" class="s1">sebastian.tschiatschek@univie.ac.at</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: right;"><a href="mailto:holger.froening@ziti.uni-heidelberg.de" class="s1">holger.froening@ziti.uni-heidelberg.de</a></p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a href="mailto:pernkopf@tugraz.at" style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 11pt;" target="_blank">Franz Pernkopf                             </a><a href="mailto:pernkopf@tugraz.at" class="s1" target="_blank">pernkopf@tugraz.at</a></p><p class="s2" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Graz University of Technology, Austria</p><p class="s2" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Laboratory of Signal Processing and Speech Communication</p><h3 style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Zoubin Ghahramani</h3><p class="s2" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">University of Cambridge, UK</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="mailto:zoubin@eng.cam.ac.uk" class="s1">zoubin@eng.cam.ac.uk</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">Abstract</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 25pt;text-indent: 0pt;text-align: justify;">While machine learning is traditionally a resource intensive task, embedded systems, autonomous navigation, and the vision of the Internet of Things fuel the interest in resource-efficient approaches. These approaches aim for a carefully chosen trade-off between performance and resource consumption in terms of computation and energy. The development of such approaches is among the major challenges in current machine learning research and key to ensure a smooth transition of machine learning technology from a scientific environment with virtually unlimited computing resources into everyday’s applications. In this article, we provide an overview of the current state of the art of machine learning techniques facilitating these real-world requirements. In particular, we focus on deep neural networks (DNNs), the predominant machine learning models of the past decade. We give a comprehensive overview of the vast literature that can be mainly split into three non-mutually exclusive categories: (i) quantized neural networks, (ii) network pruning, and (iii) structural efficiency. These techniques can be applied during training or as post-processing, and they are widely used to reduce the computational demands in terms of memory footprint, inference speed, and energy efficiency. We also briefly discuss different concepts of embedded hardware for DNNs and their compatibility with machine learning techniques as</p><p class="s4" style="padding-top: 2pt;padding-left: 25pt;text-indent: 0pt;text-align: justify;">well as potential for energy and latency reduction. We substantiate our discussion with experiments on well-known benchmark datasets using compression techniques (quantization, pruning) for a set of resource-constrained embedded systems, such as CPUs, GPUs and FPGAs. The obtained results highlight the difficulty of finding good trade-offs between resource efficiency and predictive performance.</p><h4 style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: justify;">Keywords: <span class="s4">Resource-efficient machine learning, inference, deep neural networks.</span></h4><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li data-list-text="1."><h2 style="padding-top: 6pt;padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark0">Introduction</a></h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Machine learning is a key technology in the 21<span class="s5">st</span> century and the main contributing factor for many recent performance boosts in computer vision, natural language processing, speech recognition and signal processing. Today, the main application domain and comfort zone of machine learning applications is the “virtual world”, as found in recommender systems, stock market prediction, and social media services. However, we are currently witnessing a transition of machine learning moving into “the wild”, where most prominent examples are autonomous navigation for personal transport and delivery services, and the Internet of Things (IoT). Evidently, this trend opens several real-world challenges for machine learning engineers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 117pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="292" height="208" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="padding-top: 8pt;padding-left: 4pt;text-indent: 0pt;text-align: center;"><a name="bookmark1">Figure 1: Aspects of resource-efficient machine learning models.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark1" class="a">Current machine learning approaches prove particularly effective when big amounts of data and ample computing resources are available. However, in real-world applications the computing infrastructure during the operation phase is typically limited, which effectively rules out most of the current resource-hungry machine learning approaches. There are several key challenges—illustrated in Figure </a>1—which have to be jointly considered to facilitate machine learning in real-world applications:</p><h3 style="padding-top: 5pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;">Representational efficiency <span class="p">The model complexity, i.e., the number of model parameters, should match the (usually limited) resources in deployed systems, in particular regarding memory footprint.</span></h3><h3 style="padding-top: 7pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;">Computational efficiency <span class="p">The computational cost of performing inference should match the (usually limited) resources in deployed systems, and exploit the available hardware</span></h3><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;line-height: 108%;text-align: justify;">optimally in terms of time and energy. For instance, power constraints are key for autonomous and embedded systems, as the device lifetime for a given battery charge needs to be maximized, or constraints set by energy harvesters need to be met.</p><h3 style="padding-top: 9pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;">Prediction quality <span class="p">The focus of classical machine learning is mostly on optimizing the prediction quality of the models. For embedded devices, model complexity versus prediction quality trade-offs must be considered to achieve good prediction performance while simultaneously reducing computational complexity and memory requirements.</span></h3><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark15" class="a">In this article, we review the state of the art in machine learning with regard to these real-world requirements. We focus on deep neural networks (DNNs), the currently predominant machine learning models. We formally define DNNs in Section </a>2 and give a brief introduction to the most prominent building blocks, such as dropout and batch normalization.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark45" class="a">While being the driving factor behind many recent success stories, DNNs are notoriously data and resource hungry, a property which has recently renewed significant research interest in resource-efficient approaches. This paper is dedicated to giving an extensive overview of the current directions of research of these approaches, all of which are concerned with reducing the model size and/or improving inference efficiency while at the same time maintaining accuracy levels close to state-of-the-art models. We have identified three major directions of research concerned with enhancing resource efficiency in DNNs that we present in Section </a>3. In particular, these directions are:</p><h3 style="padding-top: 9pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;">Quantized Neural Networks <span class="p">Typically, the weights of a DNN are stored as 32-bit floating-point values and during inference millions of floating-point operations are carried out. Quantization approaches reduce the number of bits used to store the weights and the activations of DNNs. While quantization approaches obviously reduce the memory footprint of a DNN, the selected weight representation potentially also facilitates faster inference using cheaper arithmetic operations. Even reducing precision down to binary or ternary values works reasonably well and essentially reduces DNNs to hardware-friendly logical circuits.</span></h3><h3 style="padding-top: 9pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;">Network Pruning <span class="p">Starting from a fixed, potentially large DNN architecture, pruning approaches remove parts of the architecture during training or after training as a post-processing step. The parts being removed range from the very local scale of individual weights—which is called unstructured pruning—to a more global scale of neurons, channels, or even entire layers—which is called structured pruning. On the one hand, unstructured pruning is typically less sensitive to accuracy degradation, but special sparse matrix operations are required to obtain a computational benefit. On the other hand, structured pruning is more delicate with respect to accuracy but the resulting data structures remain dense such that common highly optimized dense matrix operations available on most off-the-shelf hardware can be used.</span></h3><p style="padding-top: 9pt;padding-left: 33pt;text-indent: -27pt;line-height: 108%;text-align: justify;"><b>Structural Efficiency </b>This category comprises a diverse set of approaches that achieve resource efficiency at the structural level of DNNs. <i>Knowledge distillation </i>is an approach where a small student DNN is trained to mimic the behavior of a larger teacher DNN, which has been shown to yield improved results compared to training the small DNN directly. The idea of <i>weight sharing </i>is to use a small set of weights that is</p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;line-height: 108%;text-align: justify;">shared among several connections of a DNN to reduce the memory footprint. Several works have investigated <i>special matrix structures </i>that require fewer parameters and allow for faster matrix multiplications—the main workload in fully connected layers. Furthermore, there exist several <i>manually designed architectures </i>that introduced lightweight building blocks or modified existing building blocks to enhance resource efficiency. Most recently, <i>neural architecture search </i>methods have emerged that discover efficient DNN architectures automatically.</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Evidently, many of the presented techniques are not mutually exclusive, and they can potentially be combined to further enhance resource efficiency. For instance, one can both sparsify a model and reduce arithmetic precision.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark64" class="a">We complement our literature review with a brief overview of embedded hardware for DNNs in Section </a>4. These hardware platforms can be categorized into CPUs, GPUs, FPGAs and domain-specific accelerators, where each architecture exhibits different properties for deploying models. We discuss potentials and limitations of such embedded hardware with considerations on vectorization and parallelization, frequency and energy efficiency, as well as their applicability for resource-efficient models.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark74" class="a">In Section </a><a href="#bookmark76" class="a">5 we substantiate our discussion with experimental results. We provide a comparison of various quantization approaches for DNNs using the CIFAR-100 dataset in Section </a><a href="#bookmark78" class="a">5.1.1, followed by an evaluation of prediction quality for different types of pruned structures on the CIFAR-10 dataset in Section </a><a href="#bookmark80" class="a">5.1.2. We evaluate the inference throughput of the compressed models on an ARM CPU (Section </a><a href="#bookmark82" class="a">5.2.1), Xilinx FPGA (Section </a><a href="#bookmark84" class="a">5.2.2) and an embedded NVIDIA GPU (Section </a><a href="#bookmark86" class="a">5.2.3). We conclude the experiments with an overall comparison in Section </a>5.2.4, where the embedded systems (in combination with compression techniques) are studied with respect to throughput and prediction quality.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2."><h2 style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark2">Background</a><a name="bookmark15">&zwnj;</a></h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark45" class="a">Before we present a comprehensive overview of the many different techniques for reducing the complexity of DNNs in Section </a>3, this section formally introduces DNNs and some fundamentals required in the remainder of the paper.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li data-list-text="2.1"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark3">Feed-forward Deep Neural Networks</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">DNNs are typically organized in layers of alternating linear transformations and non-linear activation functions. A vanilla DNN with <i>L </i>layers is a function mapping an input <b>x</b><span class="s5">0</span> to an output <i>y </i>= <b>x</b><i>L</i><i> </i>by applying the iterative computation</p><h3 style="padding-top: 7pt;padding-left: 178pt;text-indent: 0pt;text-align: left;"><a name="bookmark16">a</a><i>l</i><i> </i><span class="p">= </span>W<i>l</i>x<i>l</i><span class="s8">−</span><span class="s5">1</span><span class="p"> + </span>b<i>l</i><i>, </i><span class="p">(1)</span><a name="bookmark17">&zwnj;</a></h3><p style="padding-top: 5pt;padding-left: 178pt;text-indent: 0pt;text-align: left;"><b>x</b><i>l</i><i> </i>= <i>φ</i>(<b>a</b><i>l</i>)<i>,                       </i>(2)</p><p class="s6" style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark16" class="a">where </a><span class="p">(1) computes a linear transformation with weight tensor </span><b>W</b><span class="s7">l</span> <span class="p">and bias vector </span><b>b</b><span class="s7">l</span><a href="#bookmark17" class="a">, and </a><span class="p">(2) computes a non-linear activation function </span>φ <span class="p">that is typically applied element-wise. Common choices for </span>φ <span class="p">are the ReLU function </span>φ<span class="p">(</span>a<span class="p">) = max(</span>a, <span class="p">0), sigmoid functions, such as tanh(</span>a<span class="p">) = (</span>e<span class="s7">a</span> <span class="s9">− </span>e<span class="s8">−</span><span class="s7">a</span><span class="p">)</span>/<span class="p">(</span>e<span class="s7">a</span> <span class="p">+ </span>e<span class="s8">−</span><span class="s7">a</span><span class="p">) and the logistic function 1</span>/<span class="p">(1 + </span>e<span class="s8">−</span><span class="s7">a</span><span class="p">), and, in the context of resource-efficient models, the sign function sign(</span>a<span class="p">) = I(</span>a <span class="s9">≥ </span><span class="p">0) </span><span class="s9">− </span><span class="p">I(</span>a &lt; <span class="p">0), where I is the indicator function.</span></p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;">In this paper, we focus on hardware-efficient machine learning in the context of classification, i.e., the task of assigning the input <b>x</b><span class="s5">0</span> to a class <i>c</i>ˆ <span class="s9">∈ {</span>1<i>, . . . , </i><span class="s9">C}</span>. Other predictive tasks, such as regression and multi-label prediction, can be tackled in a similar manner. For classification tasks, the output activation function <i>φ </i>for computing <b>x</b><i>L</i><i> </i><span class="s9">∈ </span>R<span class="s8">C</span><span class="s10"> </span>is typically the</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">c</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">softmax function <i>φ</i>(<b>a</b>)<i>i</i><i> </i>= <i>e</i><i>a</i><span class="s13">i</span><i>/ </i><span class="s14">Σ</span><span class="s15">j </span><i>e</i><i>a</i><span class="s18">j </span>. An input <b>x</b><span class="s5">0</span> is assigned to class <i>c</i>ˆ = arg max<span class="s19">c </span><i>x</i><i>L</i>.</p><p class="s5" style="padding-left: 22pt;text-indent: 0pt;line-height: 6pt;text-align: justify;"><a href="#bookmark19" class="a">The two most common types of layers are (i) fully connected layers</a>1<span class="p"> and (ii) convolu-</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">tional layers. For fully connected layers, the input <b>x </b><span class="s9">∈ </span>R<i>n</i><i> </i>is a vector whose individual dimensions—also called <a href="#bookmark20" style=" color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt;">neurons</a><span class="s5">2</span>—do not exhibit any a-priori known structure. The linear transformation of a fully connected layer is implemented as a matrix-vector multiplication <b>Wx </b>where <b>W </b><span class="s9">∈ </span>R<i>m</i><span class="s8">×</span><i>n</i>.</p><p style="padding-left: 22pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">Convolutions are used if the data exhibits spatial or temporal dimensions such as images,</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="p">in which case the DNN is called a convolutional neural network (CNN). Two-dimensional images can be represented as three-dimensional tensors </span><b>x</b><span class="s7">l</span> <span class="s9">∈ </span><span class="p">R</span><span class="s7">C</span><span class="s8">×</span><span class="s7">W</span> <span class="s8">×</span><span class="s7">H</span><span class="p">, where </span>C <span class="p">refers to the number of </span>channels <span class="p">(or, equivalently, </span>feature maps<span class="p">), and </span>W <span class="p">and </span>H <span class="p">refer to the width and the height of the image, respectively. A </span>K<span class="s12">w</span> <span class="s9">× </span>K<span class="s12">h</span> <span class="p">convolution using a rank-4 </span>filter <span class="p">weight tensor </span><b>W </b><span class="s9">∈ </span><span class="p">R</span><span class="s7">K</span><span class="s21">w</span><span class="s22">×</span><span class="s23">K</span><span class="s18">h</span><span class="s22">×</span><span class="s23">C</span><span class="s22">×</span><span class="s23">D </span><span class="p">mapping </span><b>x</b><span class="s7">l</span> <span class="s9">∈ </span><span class="p">R</span><span class="s7">C</span><span class="s8">×</span><span class="s7">W</span> <span class="s8">×</span><span class="s7">H</span> <span class="p">to </span><b>a</b><span class="s7">l</span><span class="s5">+1</span><span class="p"> </span><span class="s9">∈ </span><span class="p">R</span><span class="s7">D</span><span class="s8">×</span><span class="s7">W</span> <span class="s8">×</span><span class="s7">H</span> <span class="p">is computed as</span></p><p class="s24" style="padding-top: 5pt;padding-left: 137pt;text-indent: 0pt;line-height: 3pt;text-align: left;">K<span class="s25">w  </span>K<span class="s25">h  </span>C</p><p class="s26" style="padding-top: 6pt;text-indent: 0pt;line-height: 9pt;text-align: right;">a<span class="s27">l</span><span class="s28">+1</span></p><p class="s29" style="padding-left: 8pt;text-indent: 0pt;line-height: 55%;text-align: left;">= <span class="s30">Σ Σ Σ </span><i>W</i><span class="s32">k</span></p><p class="s11" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;line-height: 10pt;text-align: left;">,k ,c,d <span class="s33">· </span><span class="s34">x</span><span class="s35">l</span></p><p class="s6" style="padding-top: 8pt;padding-left: 88pt;text-indent: 0pt;line-height: 7pt;text-align: left;">,        <span class="p">(3)</span></p><p class="s11" style="text-indent: 0pt;line-height: 5pt;text-align: right;">d,w,h</p><p class="s36" style="text-indent: 0pt;line-height: 5pt;text-align: right;">w h</p><p class="s11" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">k<span class="s37">w</span><span class="s28">=1 </span>k<span class="s37">h</span><span class="s28">=1 </span>c<span class="s28">=1</span></p><p class="s11" style="padding-left: 25pt;text-indent: 0pt;line-height: 5pt;text-align: left;">c,i<span class="s28">(</span>w,k<span class="s37">w</span>,K<span class="s37">w</span><span class="s28">)</span>,i<span class="s28">(</span>h,k<span class="s37">h</span>,K<span class="s37">h</span><span class="s28">)</span></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where <i>i </i>is the auxiliary indexing function</p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-top: 7pt;padding-left: 159pt;text-indent: 0pt;text-align: left;">i<span class="p">(</span>p, k, K<span class="p">) = </span>p <span class="s9">−</span><span class="s38">  </span><u>K</u><span class="s38">  </span><span class="p">+ </span>k.                 <span class="p">(4)</span></p><p style="padding-top: 13pt;padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">Each spatial location of the output feature map <b>a</b><i>l</i><span class="s5">+1</span> is computed from a <i>K</i><i>w</i><i> </i><span class="s9">× </span><i>K</i><i>h</i><i> </i>region of the input image <b>x</b><i>l</i>. By using the same filter to compute the values at different spatial locations, a translation invariant detection of features is obtained. The spatial size of features detected within an image is bounded by the <i>receptive field</i>, i.e., the section of the input image that influences the value of a particular spatial location in some hidden layer. The receptive field is increased by stacking multiple convolutional layers, e.g., performing two consecutive 3 <span class="s9">× </span>3 convolutions results in each output spatial location being influenced by a larger 5 <span class="s9">× </span>5 region of the input feature maps.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">Another form of translational invariance is achieved by <i>pooling </i><a href="#bookmark21" class="a">operations that merge spatially neighboring values within a feature map to reduce the feature map’s size. Common choices are max-pooling and average-pooling which combine the results of neighboring values</a><span class="s5">3</span> by computing their maximum or average, respectively. Furthermore, pooling operations also increase the receptive field.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.2"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark4">Training of Deep Neural Networks</a><a name="bookmark18">&zwnj;</a></h3><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">The task of training is concerned with adjusting the weights <b>W </b>such that the DNN reliably predicts correct classes for unseen inputs <b>x</b><span class="s5">0</span>. This is accomplished by minimizing a loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><ol id="l3"><li data-list-text="1."><p class="s40" style="padding-left: 20pt;text-indent: -11pt;text-align: left;"><a name="bookmark19">Many popular deep learning frameworks refer to fully connected layers as </a><span class="s41">dense </span>layers.<a name="bookmark20">&zwnj;</a></p></li><li data-list-text="2."><p class="s40" style="padding-left: 20pt;text-indent: -11pt;line-height: 11pt;text-align: left;"><a name="bookmark21">For 1 </a><span class="s42">&lt; l &lt; L</span>, we speak of <span class="s41">hidden </span>layers and <span class="s41">hidden </span>neurons.</p></li><li data-list-text="3."><p class="s40" style="padding-left: 20pt;text-indent: -11pt;line-height: 11pt;text-align: left;">Typically, a 2 <span class="s43">× </span>2 region to halve the feature map size is used.</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: left;">function <span class="s9">L </span><a href="#bookmark161" class="a">using gradient-based optimization (Nocedal and Wright, </a>2006). Given some labeled training data <span class="s9">D </span>= <span class="s9">{</span>(<b>x</b><span class="s5">0</span><i>, t</i><span class="s44">1</span>)<i>, . . . , </i>(<b>x</b><span class="s5">0</span> <i>, t</i><i>N</i><i> </i>)<span class="s9">} </span>containing <i>N </i>input-target pairs, a typical</p><p class="s28" style="padding-left: 147pt;text-indent: 0pt;line-height: 35%;text-align: left;">1       <i>N</i></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">loss function has the form</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">N</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 2pt;padding-left: 124pt;text-indent: 0pt;text-align: left;"><a name="bookmark22"><span class="s9">L</span></a>(<b>W</b>; <span class="s9">D</span>) = <span class="s45">Σ </span><i>l</i>(<i>y</i>(<b>W</b><i>, </i><b>x</b><span class="s5">0</span> )<i>, t</i><i>n</i>) + <i>λr</i>(<b>W</b>)<i>, </i>(5)</p><p class="s11" style="padding-left: 4pt;text-indent: 0pt;text-align: center;">n<span class="s28">=1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">where <i>l</i>(<i>y</i><i>n</i><i>, t</i><i>n</i>) is the <i>data term </i>that penalizes the DNN parameters <b>W </b>if the output <i>y</i><i>n</i><i> </i>does not match the target value <i>t</i><i>n</i>, <i>r</i>(<b>W</b>) is a <i>regularizer </i>that prevents the DNN from overfitting, and <i>λ &gt; </i>0 is a trade-off hyperparameter. Typical choices for the data term <i>l</i>(<i>y</i><i>n</i><i>, t</i><i>n</i>) are the cross-entropy loss or the mean squared error loss, whereas typical choices for the regularizer <i>r</i>(<b>W</b>) are the <i>l</i><span class="s5">1</span>-norm or the <i>l</i><span class="s5">2</span>-norm of the weights. The loss is minimized using <i>gradient descent </i>by iteratively computing</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 158pt;text-indent: 0pt;text-align: left;"><span class="h3">W </span>← <span class="h3">W </span>− <span class="s6">η</span>∇<span class="s46">W</span>L<span class="p">(</span><span class="h3">W</span><span class="p">; </span>D<span class="p">)</span><span class="s6">,                 </span><span class="p">(6)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">where <i>η </i>is a learning rate hyperparameter. In practice, more involved <i>stochastic </i><a href="#bookmark131" class="a">gradient descent (SGD) schemes, such as ADAM (Kingma and Ba, </a>2015), are used that randomly select smaller subsets of the data—called mini-batches—to approximate the gradient.</p><p class="s9" style="padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><span class="p">Modern deep learning frameworks play an important role in the growing popularity of DNNs as they make gradient-based optimization particularly convenient: The user specifies the loss </span>L <span class="p">as a computation graph and the gradient </span>∇<span class="s46">W</span>L <a href="#bookmark172" class="a">is calculated automatically by the framework using the backpropagation algorithm (Rumelhart et al., </a><a href="#bookmark172">1986).</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="2.3"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark5">Batch Normalization</a></h3><p style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">The literature has established a consensus that using more layers improves the classification performance of DNNs. However, increasing the number of layers <i>L </i><a href="#bookmark18" class="a">also increases the difficulty of training a DNN using gradient-based methods as described in Section </a><a href="#bookmark126" class="a">2.2. Most modern DNN architecture employ batch normalization (Ioffe and Szegedy, </a>2015) after the linear transformation of some or all layers by computing</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-top: 3pt;padding-left: 53pt;text-indent: 0pt;line-height: 3pt;text-align: left;"><span class="s11">l     l                    </span>N<span class="s25">B                   </span>N<span class="s25">B</span></p><p class="s6" style="padding-left: 11pt;text-indent: 0pt;line-height: 14pt;text-align: left;">a<span class="s7">l</span>  <span class="s9">← </span><span class="s47">a</span><span class="s35">n,d </span><span class="s48">− </span><span class="s47">µ</span><span class="s35">d </span><span class="s9">· </span>γ <span class="p">+ </span>β</p><p style="padding-top: 8pt;padding-left: 11pt;text-indent: 0pt;line-height: 6pt;text-align: left;">with <i>µ</i><i>l</i></p><p class="s49" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;line-height: 25%;text-align: left;">← <span class="s50">&nbsp;</span><span class="s51">1 </span><span class="p"> </span><span class="s52">Σ </span><span class="s53">a</span><span class="s54">l</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><i>,  </i>(<i>σ</i><i>l</i><i> </i>)<span class="s5">2</span> <span class="s9">← </span><span class="s55">&nbsp; </span><u>1  </u><span class="s57"> </span><span class="s45">Σ</span>(<i>a</i><i>l</i></p><p class="s6" style="padding-top: 6pt;padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s9">— </span>µ<span class="s7">l</span> <span class="p">)</span><span class="s5">2</span>,</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="text-indent: 0pt;line-height: 11pt;text-align: left;">σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 16pt;text-indent: 0pt;line-height: 9pt;text-align: left;">n,d</p><p class="s19" style="padding-left: 17pt;text-indent: 0pt;line-height: 79%;text-align: left;">l    <span class="s11">d   d</span></p><p class="s11" style="padding-left: 16pt;text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p class="s11" style="padding-left: 16pt;text-indent: 0pt;line-height: 69%;text-align: left;">d <span class="s26">N</span><span class="s58">B</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">n<span class="s28">=1</span></p><p class="s11" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">n,d   d</p><p class="s6" style="padding-left: 16pt;text-indent: 0pt;line-height: 17pt;text-align: left;">N<span class="s12">B</span> <span class="s9">− </span><span class="p">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">n<span class="s28">=1</span></p><p class="s11" style="padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: left;">n,d</p><p class="s11" style="text-indent: 0pt;line-height: 9pt;text-align: center;">d</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 24pt;text-indent: 0pt;text-align: left;">(7)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><span class="p">where </span>β<span class="s12">d</span> <span class="p">and </span>γ<span class="s12">d</span> <span class="p">are trainable parameters, and </span>N<span class="s12">B</span> <span class="p">is the mini-batch size of SGD.</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The idea is to normalize the activation statistics over the data samples in each layer to zero mean and unit variance. This results in similar activation statistics throughout the network which facilitates gradient flow during backpropagation. The linear transformation of the normalized activations with the parameters <i>β </i>and <i>γ </i><a href="#bookmark54" class="a">is mainly used to recover the DNNs ability to approximate any desired function—a feature that would be lost if only the normalization step is performed. Most recent DNN architectures have been shown to benefit from batch normalization, and, as reviewed in Section </a>3.2.2, batch normalization can be targeted to achieve resource efficiency in DNNs.</p></li><li data-list-text="2.4"><h3 style="padding-top: 1pt;padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark6">Dropout</a></h3><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="#bookmark179" class="a">Dropout as introduced by Srivastava et al. </a>(2014) is a way to prevent neural networks from</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">overfitting by injecting multiplicative noise to the inputs of a layer, i.e., <i>x</i><i>l</i></p><p class="s9" style="padding-left: 4pt;text-indent: 0pt;line-height: 15pt;text-align: left;">← <span class="s6">x</span><span class="s7">l</span></p><p class="s9" style="padding-left: 3pt;text-indent: 0pt;line-height: 15pt;text-align: left;">· <span class="s6">ε</span><span class="p">. A</span></p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 0pt;line-height: 15pt;text-align: left;">common choice for the injected noise is <i>ε </i><span class="s9">∼ </span>Bernoulli(<i>p</i>) where the values <i>x</i><i>l</i></p><p style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">are randomly</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">set to zero with probability 1 <span class="s9">− </span><i>p</i>. Another common choice is Gaussian dropout where we</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 91%;text-align: justify;">have <i>ε </i><span class="s9">∼ N </span>(1<i>, α</i><a href="#bookmark23" class="a">).</a><span class="s5">4</span> Intuitively, the idea is that hidden neurons cannot rely on the presence of features computed by other neurons. Consequently, individual neurons are expected to</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark55" class="a">compute in a sense “meaningful” features on their own. This avoids that multiple neurons jointly compute features in an entangled way. Dropout has been cast into a Bayesian framework which was subsequently exploited to perform network pruning as detailed in Section </a><a href="#bookmark55">3.2.3.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.5"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark7">Modern Architectures</a></h3><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">As mentioned in the beginning of this section, most architectures follow the simple scheme of repeating several layers of linear transformation followed by a non-linear function <i>φ</i>. Although most successful architectures follow this scheme, recent architectures have introduced additional components and subtle extensions that have led to new design principles. In the following, we give a brief overview of the most prominent architectures that have emerged over the past years in chronological order.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l4"><li data-list-text="2.5.1"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark8">AlexNet</a></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark134" class="a">The AlexNet architecture (Krizhevsky et al., </a>2012) was the first work to show that DNNs are capable of improving performance over conventional hand crafted computer vision techniques by achieving 16.4% Top-5 error on the ILSVRC12 challenge—an improvement of approximately 10% absolute error compared to the second best approach in the challenge which relied on well-established computer vision techniques. This most influential work essentially started the advent of DNNs, which can be seen from the fact that DNNs have spread over virtually any scientific field and achieved improved performances over well-established methods in the respective fields.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The architecture consists of eight layers—five convolutional layers followed by three fully connected layers. AlexNet was designed to optimally utilize the available hardware at that time rather than following some clear design principle. This involves the choice of</p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="p">heterogeneous window sizes </span>K<span class="s12">w</span> <span class="s9">× </span>K<span class="s12">h</span> <span class="p">and seemingly arbitrary numbers of channels per layer</span></p><p class="s6" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">C<span class="p">. Furthermore, convolutions are performed in two parallel paths to facilitate the training</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">on two GPUs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.5.2"><p class="s59" style="padding-left: 37pt;text-indent: -31pt;text-align: justify;"><a name="bookmark9">VGGNet</a></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark177" class="a">The VGGNet architecture (Simonyan and Zisserman, </a>2015) won the second place at the ILSVRC14 challenge with 7.3% Top-5 error. Compared to AlexNet, its structure is more uniform and with up to 19 layers much deeper. The design of VGGNet is guided by two main principles. (i) VGGNet uses mostly 3 <span class="s9">× </span>3 convolutions and it increases the receptive field by stacking several of them. (ii) After downscaling the spatial dimension with 2 <span class="s9">× </span>2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><ol id="l5"><li data-list-text="4."><p class="s40" style="padding-top: 1pt;padding-left: 20pt;text-indent: -11pt;text-align: left;"><a name="bookmark23">The parameters </a><span class="s42">p </span>and <span class="s42">α </span>are hyperparameters.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">max-pooling, the number of channels should be doubled to avoid information loss. From a hardware perspective, VGGNet is often preferred over other architectures due to its uniform architecture.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="2.5.3"><p class="s59" style="padding-top: 7pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark10">InceptionNet</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark181" class="a">InceptionNet (or, equivalently, GoogLeNet) (Szegedy et al., </a>2015) won the ILSVRC14 challenge with 6.7% Top-5 error with an even deeper architecture consisting of 22 layers. The main feature of this architecture is the inception module which combines the outputs of 1 <span class="s9">× </span>1, 3 <span class="s9">× </span>3, and 5 <span class="s9">× </span>5 convolutions by stacking them. To reduce the computational burden, InceptionNet performs 1 <span class="s9">× </span><a href="#bookmark143" class="a">1 convolutions as proposed in (Lin et al., </a>2014a) to reduce the number of channels immediately before the larger 3 <span class="s9">× </span>3 and 5 <span class="s9">× </span>5 convolutions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.5.4"><p class="s59" style="padding-top: 8pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark11">ResNet</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Motivated by the observation that adding more layers to very deep conventional CNN architectures does not necessarily reduce the <i>training </i><a href="#bookmark116" class="a">error, residual networks (ResNets) introduced by He et al. </a>(2016) follow a rather different principle. The key idea is that every layer computes a residual that is <i>added </i>to the layer’s input. This is often graphically depicted as a residual path with an attached skip connection.</p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The authors hypothesize that identity mappings play an important role. They argue that it is easier to model identity mappings in ResNets by simply setting all the weights of the residual path to zero instead of simulating them by adapting the weights of several consecutive layers in an intertwined way. In any case, the skip connections reduce the vanishing gradient problem during training and enable extremely deep architectures of up to 152 layers on ImageNet and even up to 1,000 layers on CIFAR-10. ResNet won the ILSVRC15 challenge with 3.6% Top-5 error.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="2.5.5"><p class="s59" style="padding-top: 7pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark12">DenseNet</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark122" class="a">Inspired by ResNets whose skip connections have shown to reduce the vanishing gradient problem, densely connected CNNs (DenseNets) introduced by Huang et al. </a>(2017) drive this idea even further by connecting each layer to <i>all </i>previous layers. DenseNets are conceptually very similar to ResNets—instead of adding the output of a layer to its input, DenseNets <i>stack </i>the output and the input of each layer. Since this stacking necessarily increases the number of feature maps with each layer, the number of new feature maps computed by each layer is typically small. Furthermore, it is proposed to use compression layers after downscaling the spatial dimension with pooling, i.e., a 1 <span class="s9">× </span>1 convolution is used to reduce the number of feature maps.</p><p style="padding-top: 8pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">Compared to ResNets, DenseNets achieve similar performance, allow for even deeper architectures, and they are more parameter and computation efficient. However, the DenseNet architecture is highly non-uniform which complicates the hardware mapping and ultimately slows down training.</p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark24"><span style=" color: black; font-family:&quot;Arial Black&quot;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt;">W</span></a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">l</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;"><span style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt;">Q</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;"><span style=" color: #F00; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt;">Forward path</span></p><p style="padding-top: 2pt;text-indent: 0pt;text-align: left;"><span style=" color: #007F00; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt;">Backward path</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;"><span style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt;">id</span></p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><span style=" color: black; font-family:&quot;Arial Black&quot;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt;">W</span><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">l</span></p><p style="text-indent: 0pt;line-height: 6pt;text-align: left;"><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">q</span></p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><span style=" color: black; font-family:&quot;Arial Black&quot;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt;">x</span><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">l</span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: left;"><span style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt;">conv</span></p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><span style=" color: black; font-family:&quot;Arial Black&quot;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt;">a</span><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">l</span><span style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt;">+1</span></p><p style="text-indent: 0pt;line-height: 11pt;text-align: left;"><span style=" color: black; font-family:&quot;Arial Black&quot;, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; vertical-align: -3pt;">x</span><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt;">l</span><span style=" color: black; font-family:Georgia, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt;">+1</span></p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="19" height="17" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAARCAYAAAA/mJfHAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABaUlEQVQ4jY2TMWvCQBTHj6M5tR3sEzqUDAXXazeXQodshUTrB2jXDF3aUT9BM+omrvoBAqYJuDgIbl00a8DBQRxOJ88cDZdOHc3dwZvejx/3/u8O5XmOdEtKiZIkqbuuO6SUxgDACCEpxjjzPK+jLcrzHM3n86dqtXrodrtfi8Xicb1e3+12uxvGGJxOp5K2iHNeNk1zMx6PX88x2rJer/dpWdZMSnmW0c6KUhr7vt8u4rRky+XyAQCYEMIo4jDSOJPJ5MVxnJAQ8lvEacmiKLKbzea3ElSNuN/vrw3DEIwxULHKm02n0+dGo/FTq9UOKlYpC8PQsW07Uo6oGlMIYQAAW61W9zpbL2z6vt+mlMZFD1VLJqVElmXN+v3+h+4vOdsYjUZvpmluOOdlXdlFmqYlzvlllmUXx+Pxarvd3gZB0BoMBu9BELQqlUqqFT5CCHme18EYZ4SQFAAYpTR2XXeYJEldN6v/+gO4paSAXCKIoAAAAABJRU5ErkJgggAA"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Figure 2: A simplified building block of a DNN using the straight-through gradient estimator (STE). <i>Q </i>denotes some arbitrary piecewise constant quantization function and <i>id </i>denotes the identity function which simply passes the gradient on during backpropagation. In the forward pass, the solid red line is followed which passes the two piecewise constant functions <i>Q </i>and sign whose gradient is zero almost everywhere (red boxes). During backpropagation, the dashed green line is followed which avoids these piecewise constant functions and instead only passes differentiable functions (green boxes)—in particular, the functions <i>id </i>and tanh whose shapes are similar to <i>Q </i>and sign but whose gradient is non-zero. This allows us to obtain an approximate non-zero gradient for the real-valued parameters <b>W</b><i>l</i><i> </i>(blue circle) which are subsequently updated with SGD.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="2.6"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark13">The Straight-Through Gradient Estimator</a><a name="bookmark25">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">Many recently developed methods for resource efficiency in DNNs incorporate components in the computation graph of the loss function <span class="s9">L </span><a href="#bookmark18" class="a">that are non-differentiable or whose gradient is zero almost everywhere, such as piecewise constant quantizers. These components prevent the use of conventional gradient-based optimization as described in Section </a><a href="#bookmark18">2.2.</a></p><p class="s6" style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><span class="p">The straight-through gradient estimator (STE) is a simple but effective way to approximate the gradient of such components by simply replacing their gradient with a non-zero value. Let </span>f <span class="p">(</span>w<span class="p">) be some non-differentiable operation within the computation graph of </span><span class="s9">L </span><span class="p">such that the partial derivative </span>∂<span class="s9">L</span>/∂w <span class="p">is not defined. The STE then approximates the gradient </span>∂<span class="s9">L</span>/∂w <span class="p">by</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-left: 165pt;text-indent: 0pt;line-height: 13pt;text-align: left;">∂<span class="s9">L  </span>∂<span class="s9">L </span>∂f</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 184pt;text-indent: 0pt;line-height: 6pt;text-align: left;">=</p><p class="s6" style="padding-left: 165pt;text-indent: 0pt;line-height: 10pt;text-align: left;">∂w  ∂f ∂w</p><p class="s6" style="padding-top: 9pt;padding-left: 15pt;text-indent: 0pt;line-height: 13pt;text-align: left;">∂<span class="s9">L </span>∂f<span class="s60">˜</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s9" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: left;">≈</p><p class="s6" style="padding-left: 15pt;text-indent: 0pt;line-height: 9pt;text-align: left;">∂f ∂w</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 7pt;text-indent: 0pt;text-align: left;">,                 <span class="p">(8)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <i>f</i><span class="s60">˜</span>(<i>w</i>) is an arbitrary differentiable function with a similar functional shape as <i>f </i>(<i>w</i>). For instance, in case of the sign activation function <i>f </i>(<i>w</i>) = sign(<i>w</i>) whose derivative is zero almost everywhere, one could select <i>f</i><span class="s60">˜</span>(<i>w</i>) = tanh(<i>w</i>). Another common choice is the identity function <i>f</i><span class="s60">˜</span>(<i>w</i>) = <i>w </i>whose derivative is <i>f</i><span class="s60">˜</span><span class="s22">r</span>(<i>w</i><a href="#bookmark24" class="a">) = 1, which simply passes the gradient on to higher components in the computation graph during backpropagation. Figure </a>2 illustrates the STE applied to a simplified DNN layer.</p></li><li data-list-text="2.7"><h3 style="padding-top: 1pt;padding-left: 28pt;text-indent: -22pt;text-align: justify;"><a name="bookmark14">Bayesian Neural Networks</a><a name="bookmark26">&zwnj;</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">Since there exist several works for resource-efficient DNNs that build on the framework of Bayesian neural networks, we briefly introduce the basic principles here. Given a prior distribution <i>p</i>(<b>W</b>) over the weights and a likelihood <i>p</i>(<span class="s9">D| </span><b>W</b>) defined by the softmax output of a DNN as</p><p style="text-indent: 0pt;line-height: 23pt;text-align: left;"><i>p</i>(<span class="s9">D| </span><b>W</b>) = <span class="s45">Y </span><i>p</i>(<i>y</i>(<b>W</b><i>, </i><b>x </b>) = <i>t</i><i>n</i>)<i>,             </i>(9)</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-top: 6pt;text-indent: 0pt;text-align: center;">N</p><p class="s28" style="padding-left: 77pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p><p class="s11" style="padding-left: 78pt;text-indent: 0pt;line-height: 8pt;text-align: center;">n</p><p class="s11" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">n<span class="s28">=1</span></p><p style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: center;">we can use Bayes’ rule to infer a posterior distribution over the weights, i.e.,</p><p style="padding-top: 5pt;padding-left: 4pt;text-indent: 0pt;line-height: 11pt;text-align: center;"><i>p</i>(<span class="s9">D| </span><b>W</b>) <i>p</i>(<b>W</b>)</p><p style="text-indent: 0pt;text-align: left;"/><p style="text-indent: 0pt;line-height: 13pt;text-align: right;"><i>p</i>(<b>W </b><span class="s9">|D</span>) =</p><p style="padding-top: 3pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><i>p</i>(<span class="s9">D</span>)</p><p style="padding-left: 26pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><a name="bookmark27"><span class="s9">∝ </span></a><i>p</i>(<span class="s9">D| </span><b>W</b>) <i>p</i>(<b>W</b>)<i>. </i>(10)</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: left;">From a Bayesian perspective it is desired to compute expected predictions with respect to the posterior distribution, i.e.,</p><p style="padding-top: 9pt;padding-left: 175pt;text-indent: 0pt;text-align: left;"><a name="bookmark28">E</a><span class="s19">p</span><span class="s61">(</span><span class="s62">W </span><span class="s63">|D</span><span class="s61">)</span><span class="s64">[</span><i>y</i>(<b>W</b><i>, </i><b>x</b><span class="s5">0</span>)]<i>, </i>(11)</p><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">and not just to reduce the entire distribution to a single point estimate. However, due to the highly non-linear nature of DNNs, most exact inference scenarios involving the full posterior <i>p</i>(<b>W </b><span class="s9">|D</span><a href="#bookmark117" class="a">) are typically intractable and there exist a range of approximation techniques for these tasks, such as variational inference (Hinton and van Camp, </a><a href="#bookmark108" class="a">1993; Graves, </a><a href="#bookmark93" class="a">2011; Blundell et al., </a><a href="#bookmark160" class="a">2015) and sampling based approaches (Neal, </a><a href="#bookmark160">1992).</a></p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;line-height: 86%;text-align: justify;">Interestingly, training DNNs can often be seen as a very rough Bayesian approximation where we only seek for weights <b>W </b>that maximize the posterior <i>p</i>(<b>W </b><span class="s9">|D</span>), which is also known as maximum a-posteriori estimation (MAP). In particular, in a typical loss <span class="s9">L </span><a href="#bookmark22" class="a">as in </a>(5) the data term originates from the logarithm of the likelihood <i>p</i>(<span class="s9">D| </span><b>W</b>) whereas the regularizer originates from the logarithm of the prior <i>p</i>(<b>W</b>).</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 16pt;line-height: 88%;text-align: justify;">A better Bayesian approximation is obtained with variational inference where the aim is to find a variational distribution <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) governed by distribution parameters <i>ν </i>that is as close as possible to the posterior <i>p</i>(<b>W </b><span class="s9">|D</span>) but still simple enough to allow for efficient inference, e.g., for computing E<span class="s19">q</span><span class="s61">(</span><span class="s62">W </span><span class="s63">|</span><span class="s19">ν</span><span class="s61">)</span><span class="s64">[</span><i>y</i>(<b>W</b><i>, </i><b>x</b><span class="s5">0</span>)] by sampling from <i>q</i>. This is typically achieved by the so called <i>mean field </i>assumption, i.e., by assuming that the weights are independent such that <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) factorizes into a product of factors <i>q</i>(<i>w </i><span class="s9">| </span><i>ν</i><i>w</i>) for each weight <i>w </i><span class="s9">∈ </span><b>W</b>. The most prominent approach to obtain the variational distribution <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) is by minimizing the KL-divergence KL(<i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>)<span class="s9">||</span><i>p</i>(<b>W </b><span class="s9">|D</span><a href="#bookmark167" class="a">)) using gradient-based optimization (Ranganath et al., </a><a href="#bookmark93" class="a">2014; Blundell et al., </a><a href="#bookmark93">2015).</a></p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The Bayesian approach is appealing as distributions over the parameters directly translate into predictive distributions. In contrast to ordinary DNNs that only provide a point estimate prediction, Bayesian neural networks offer predictive uncertainties which are useful to determine how certain the DNN is about its own prediction. However, the Bayesian framework has got several other useful properties that can be exploited to obtain resource-efficient DNNs. For instance, the prior <i>p</i>(<b>W</b><a href="#bookmark49" class="a">) allows us to incorporate information about properties, such as sparsity, that we expect to be present in the DNN. In Section </a><a href="#bookmark55" class="a">3.1.3, we review weight quantization approaches based on the Bayesian paradigm, and in Section </a>3.2.3, we review pruning approaches based on the Bayesian paradigm.</p></li></ol></li><li data-list-text="3."><h2 style="padding-top: 1pt;padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark29">Resource Efficiency in Deep Neural Networks</a><a name="bookmark45">&zwnj;</a></h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">In this section, we provide a comprehensive overview of methods that enhance the efficiency of DNNs regarding memory footprint, computation time, and energy requirements. We have identified three different major approaches that aim to reduce the computational complexity of DNNs, i.e., (i) weight and activation quantization, (ii) network pruning, and (iii) structural efficiency. These categories are not mutually exclusive, and we present individual methods in the category where their contribution is most significant.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l6"><li data-list-text="3.1"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark30">Quantized Neural Networks</a><a name="bookmark46">&zwnj;</a></h3><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">Quantization in DNNs is concerned with reducing the number of bits used for the representation of the weights and the activations. The reduction in memory requirements are obvious: Using fewer bits for the weights results in a lower memory overhead for storing the corresponding model, and using fewer bits for the activations results in a lower memory overhead for computing predictions. Furthermore, representations using fewer bits often facilitate faster computation. For instance, when quantization is driven to the extreme with binary weights <i>w </i><span class="s9">∈ {−</span>1<i>, </i>1<span class="s9">} </span><i>and </i>binary activations <i>x </i><span class="s9">∈ {−</span>1<i>, </i>1<span class="s9">}</span>, floating-point or fixed-point dot products are replaced by hardware-friendly logical XNOR and bitcount operations. In this way, a sophisticated DNN is essentially reduced to a logical circuit.</p><p class="s5" style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark47" class="a">However, training such discrete-valued DNNs</a>5<span class="p"> is difficult as they cannot be directly optimized using gradient-based methods. The challenge is to reduce the number of bits as much as possible while at the same time keeping the prediction accuracy close to that of a well-tuned full-precision DNN. In the sequel, we provide a literature overview of approaches that train reduced-precision DNNs, and, in a broader view, we also consider methods that use reduced-precision computations during backpropagation to facilitate low-resource training.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l7"><li data-list-text="3.1.1"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark31">Early Quantization Approaches</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark119" class="a">Approaches for reduced-precision computations date back at least to the early 1990s. The two works of H¨ohfeld and Fahlman (H¨ohfeld and Fahlman, </a><a href="#bookmark120" class="a">1992a; H¨ohfeld and Fahlman, </a><a href="#bookmark111" class="a">1992b) rounded the weights during training to fixed-point format with different numbers of bits. They observed that training eventually stalls as small gradient updates are always rounded to zero. As a remedy, they proposed stochastic rounding, i.e., rounding values to the nearest value with a probability proportional to the distance to the nearest value. These quantized gradient updates are correct in expectation, do not cause training to stall, and yield good performance with substantially fewer bits than deterministic rounding. More recently, Gupta et al. </a>(2015) have shown that stochastic rounding can also be applied to modern deep architectures, as demonstrated on a hardware prototype.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;text-align: justify;"><a href="#bookmark146" class="a">Lin et al. </a>(2015) propose a method to reduce the number of multiplications required during training. At forward propagation, the weights are stochastically quantized to either binary weights <i>w </i><span class="s9">∈ {−</span>1<i>, </i>1<span class="s9">} </span>or ternary weights <i>w </i><span class="s9">∈ {−</span>1<i>, </i>0<i>, </i>1<span class="s9">} </span>to remove the need for multiplications at all. During backpropagation, inputs and hidden neurons are quantized to powers of two, reducing multiplications to cheaper bit-shift operations, and leaving only a</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"/><ol id="l8"><li data-list-text="5."><p class="s40" style="padding-top: 1pt;padding-left: 20pt;text-indent: -11pt;line-height: 107%;text-align: left;"><a name="bookmark47">Due to finite precision of computer arithmetic, in fact any DNN is discrete-valued. However, we use this term here to emphasize the extremely small number of values.</a></p></li></ol><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">negligible number of floating-point multiplications to be computed. However, the speed-up is limited to training since for testing the full-precision weights are required.</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark99" class="a">Courbariaux et al. </a><a href="#bookmark141" class="a">(2015a) empirically studied the effect of different numeric formats (i.e., floating-point, fixed-point, and dynamic fixed-point) with varying bit widths on the performance of DNNs. Lin et al. </a>(2016) consider fixed-point quantization of pre-trained full-precision DNNs. They formulate a convex optimization problem to minimize the total number of bits required to store the weights and the activations under the constraint that the total output signal-to-quantization noise ratio is larger than a certain prespecified value. A closed-form solution of the convex objective yields layer-specific bit widths.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.1.2"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark32">Quantization-aware Training</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark92" class="a">Quantization operations, being piecewise constant functions with either undefined or zero derivatives, are not applicable to gradient-based learning using backpropagation. In recent years, the STE (Bengio et al., </a><a href="#bookmark25" class="a">2013) (see Section </a>2.6) became the method of choice to compute an approximate gradient for training DNNs with weights that are represented using a very small number of bits. Such methods typically maintain a set of full-precision weights that are quantized during forward propagation. During backpropagation, the gradients are propagated through the quantization functions by assuming that their gradient equals one. In this way, the full-precision weights are updated using gradients computed at the quantized weights. At test-time, the full-precision weights are abandoned and only the quantized reduced-precision weights are kept. We term this scheme <i>quantization-aware training </i>since quantization is an essential part during forward-propagation and it is intuitive to think of the real-valued weights becoming robust to quantization. In a similar manner, many methods employ the STE to approximate the gradient for the quantization of activations.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark100" class="a">In (Courbariaux et al., </a>2015b), binary weight DNNs are trained using the STE to get rid of expensive floating-point multiplications. They consider deterministic rounding using the sign function and stochastic rounding using probabilities determined by the hard sigmoid function max(0<i>, </i>min(1<i>, </i>(<i>w </i>+ 1)<i>/</i><a href="#bookmark124" class="a">2)). During backpropagation, a set of auxiliary full-precision weights is updated based on the gradients of the quantized weights. Hubara et al. </a>(2016) extended this work by also quantizing the activations to a single bit using the sign activation function. This reduces the computational burden dramatically as floating-point multiplications and additions are reduced to hardware-friendly logical XNOR and bitcount operations, respectively.</p><p class="s6" style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark137" class="a">Li et al. </a><span class="p">(2016) trained ternary weights </span>w <span class="s9">∈ {−</span>a, <span class="p">0</span>, a<span class="s9">}</span><span class="p">. Their quantizer sets weights whose magnitude is lower than a certain threshold ∆ to zero, while the remaining weights are set to </span><span class="s9">−</span>a <span class="p">or </span>a <span class="p">according to their sign. Their approach determines </span>a &gt; <span class="p">0 and ∆ </span>during forward propagation <a href="#bookmark203" class="a">by approximately minimizing the squared quantization error of the real-valued weights. Zhu et al. </a><span class="p">(2017) extended this work to ternary weights </span>w <span class="s9">∈ {−</span>a, <span class="p">0</span>, b<span class="s9">} </span><span class="p">where </span>a &gt; <span class="p">0 and </span>b &gt; <span class="p">0 are trainable parameters subject to gradient updates. They propose to select ∆</span><span class="s7">l</span> <span class="p">based on the maximum full-precision weight magnitude in each layer, i.e.,</span></p><p class="s9" style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><span class="p">∆</span><span class="s7">l</span><span class="s6"> </span><span class="p">= </span><span class="s6">t </span>· <span class="p">max</span>{|<span class="s6">w</span>| <span class="p">: </span><span class="s6">w </span>∈ <span class="h3">W</span><span class="s7">l</span>} <span class="p">with </span><span class="s6">t </span><a href="#bookmark137" class="a">being a hyperparameter. These asymmetric weights considerably improve performance compared to symmetric weights as used by Li et al. </a><a href="#bookmark137">(2016).</a></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark168" class="a">Rastegari et al. </a>(2016) approximate full-precision weight filters in CNNs by <b>W </b>= <i>α</i><b>B </b>where <i>α </i>is a scalar and <b>B </b>is a binary weight matrix. This reduces the bulk of floating-point multiplications inside the convolutions to either additions or subtractions and only requires a single multiplication per output neuron with the scalar <i>α</i>. In a further step, the layer inputs <b>x</b><i>l</i><i> </i><a href="#bookmark145" class="a">are quantized in a similar way to perform the convolution using only efficient XNOR and bitcount operations, followed by two floating-point multiplications per output neuron. Again, the STE is used during backpropagation. Lin et al. </a><a href="#bookmark168" class="a">(2017b) generalized the ideas of Rastegari et al. </a>(2016) by approximating the full-precision weights using linear combinations of <i>multiple </i>binary weight filters for improved classification accuracy.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark96" class="a">While most activation binarization methods use the sign function which can be seen as an approximation to the tanh function, Cai et al. </a>(2017) proposed a half-wave Gaussian quantization that more closely resembles the predominant ReLU activation function.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark158" class="a">Motivated by the fact that weights and activations typically exhibit a non-uniform distribution, Miyashita et al. </a><a href="#bookmark201" class="a">(2016) proposed to quantize values to powers of two. Their representation allows getting rid of expensive multiplications, and they report higher robustness to quantization than linear rounding schemes using the same number of bits. Zhou et al. </a>(2017) proposed incremental network quantization where the weights of a pre-trained DNN are first partitioned into two sets. The weights in the first set are quantized to either zero or powers of two. The weights in the second set are kept at full precision and retrained to recover from the potential accuracy degradation due to quantization. They iterate partitioning, quantization, and retraining until all weights are quantized.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: left;"><a href="#bookmark127" class="a">Jacob et al. </a>(2018) proposed a quantization scheme that accurately approximates floating-point operations using only integer arithmetic to speed up computation. During training, their forward pass simulates the quantization step to keep the performance of the quantized DNN close to the performance of using single-precision. At test-time, weights are represented as 8-bit integer values, reducing the memory footprint by a factor of four.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark148" class="a">Liu et al. </a>(2018) introduced Bi-Real net, a ResNet-inspired architecture where the residual path is implemented with efficient binary convolutions while the shortcut path is kept real-valued to preserve the expressiveness of the DNN. The residual in each layer is computed by first transforming the input with the sign activation, followed by a binary convolution, and a final batch normalization step.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;">Instead of using a fixed quantizer, in <a href="#bookmark199" style=" color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt;">LQ-Net </a><a href="#bookmark199" class="a">(Zhang et al., </a>2018a) the quantizer is adapted during training. The proposed quantizer is inspired by the representation of integers as linear combinations <b>v</b><span class="s8">T</span><b>b </b>with <b>v </b>= (2<span class="s5">0</span><i>, . . . , </i>2<i>K</i><span class="s8">—</span><span class="s5">1</span>) and <b>b </b><span class="s9">∈ {</span>0<i>, </i>1<span class="s9">}</span><i>K</i>. The key idea is to consider a quantizer that assigns values to the nearest value representable as such a linear combination <b>v</b><span class="s8">T</span><b>b </b>and to treat <b>v </b><span class="s9">∈ </span>R<i>K</i><i> </i>as trainable parameters. It is shown that such a quantizer is compatible with efficient bit operations. The quantizer is optimized <i>during forward propagation </i>by minimizing the quantization error objective <span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="4" height="15" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAPCAYAAADDNm69AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAI0lEQVQImWNgYGA4zoAA65kYGBg4kQT4mRjQwBAX+InE/wwAmFIDl3SAhBsAAAAASUVORK5CYIIA"/></td></tr></table></span><span class="s65"> </span><b>Bv </b><span class="s9">− </span><b>x </b><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="4" height="15" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAPCAYAAADDNm69AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAI0lEQVQImWNgYGA4zoAA65kYGBg4kQT4mRjQwBAX+InE/wwAmFIDl3SAhBsAAAAASUVORK5CYIIA"/></td></tr></table></span><span class="s66"> </span><span class="s67">2</span><span class="s68"> </span>for <b>B </b><span class="s9">∈ {−</span>1<i>, </i>1<span class="s9">}</span><i>N</i><span class="s8">×</span><i>K</i><i> </i>and <b>v </b>by alternately fixing <b>B </b>and minimizing <b>v </b>and vice versa. It is proposed to use layer-wise quantizers for the activations and channel-wise quantizers for the weights, i.e., an individual quantizer for each layer and channel, respectively.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark153" class="a">Relaxed Quantization (Louizos et al., </a>2019) introduces a stochastic differentiable soft rounding scheme. By injecting additive noise to the deterministic weights before rounding, one can compute probabilities of the weights being rounded to specific values in a predefined discrete set. Subsequently, these probabilities are used to differentiably round the weights</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark129" class="a">using the Gumbel-softmax approximation (Jang et al., </a>2017). Since this soft rounding scheme produces only values that are close to values from the discrete set but which are not exactly from this set, the authors also propose a hard variant using the STE.</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark105" class="a">Dong et al. </a>(2019) introduced Hessian-aware mixed-precision quantization for DNNs. Their method quantifies the sensitivity of individual DNN blocks to weight quantization using the largest eigenvalue of the block-wise Hessian matrices which can be computed using the power iteration method. They compute two different orderings of the individual DNN blocks, both of which are based on these eigenvalues. The first ordering determines a relative ordering of the bit widths of individual blocks. This substantially reduces the exponential search space of layer-specific weights and allows them to manually set appropriate bit widths. The second ordering takes these bit widths into account and determines the sequence in which blocks are quantized and fine-tuned using quantization-aware training.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">A linear quantizer has three characteristic properties, i.e., (i) a step size <i>Q</i><i>d</i>, (ii) a dynamic range <i>Q</i><span class="s44">max</span>, and (iii) the number of bits <i>Q</i><i>b</i>. Since these quantities are interrelated according to</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 7pt;padding-left: 166pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">Q</a><span class="s44">max</span><span class="p"> = (2</span><span class="s7">Q</span><span class="s21">b</span><span class="s22">—</span><span class="s69">1 </span><span class="s9">− </span><span class="p">1)</span>Q<span class="s12">d</span>, <span class="p">(12)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark185" class="a">a linear quantizer is specified by knowing any two of them (Uhlich et al., </a>2020). Given fixed</p><p class="s6" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="p">layerwise bit widths </span>Q<span class="s7">l</span> <a href="#bookmark106" class="a">, Esser et al. </a><span class="p">(2020) incorporated layerwise step sizes </span>Q<span class="s7">l</span></p><p style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">as trainable</p><p class="s11" style="text-indent: 0pt;line-height: 7pt;text-align: center;">b</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">parameters in the computation graph. By training the step sizes <i>Q</i><i>l</i></p><p class="s11" style="text-indent: 0pt;line-height: 7pt;text-align: center;">d</p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 12pt;text-align: left;">using the STE, they</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">are adapted to the given objective. This is in contrast to previous work, such as XNOR-Net</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="#bookmark168" class="a">(Rastegari et al., </a>2016), that determine the step size <i>Q</i><i>l</i></p><p style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">using certain statistics obtained</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: left;"><a href="#bookmark185" class="a">from the values to be quantized. Uhlich et al. </a>(2020) extended this idea to mixed-precision quantization. They investigated the three different possibilities to specify a linear quantizer</p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">(12) by only two of its characteristic properties and discovered substantial differences in the</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">training behavior. They propose to parameterize the quantizers using the step size <i>Q</i><i>l</i></p><p style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">and</p><p class="s28" style="text-indent: 0pt;line-height: 8pt;text-align: left;">max</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">the dynamic range <i>Q</i><i>l</i></p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">and to train these values using backpropagation and the STE to</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">b</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;"><span class="p">obtain layerwise bit widths </span>Q<span class="s7">l</span> <span class="p">.</span></p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">There also exist works that perform quantization <i>during </i><a href="#bookmark202" class="a">backpropagation to facilitate resource-efficient training. Zhou et al. </a><a href="#bookmark193" class="a">(2016) presented several quantization schemes for the weights and the activations that allow for flexible bit widths. Furthermore, they also propose a quantization scheme for backpropagation to facilitate low-resource training. In accordance with earlier work mentioned above, they note that stochastic quantization is essential for their approach. In (Wu et al., </a><a href="#bookmark202" class="a">2018b), weights, activations, weight gradients, and activation gradients are subject to customized quantization schemes that allow for variable bit widths and facilitate integer arithmetic during training and testing. In contrast to (Zhou et al., </a><a href="#bookmark193" class="a">2016), the work of Wu et al. </a>(2018b) accumulates weight changes to low-precision weights instead of full-precision weights.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark138" class="a">While most work on quantization based approaches is empirical, some works gained more theoretical insights (Li et al., </a><a href="#bookmark90" class="a">2017; Anderson and Berg, </a><a href="#bookmark176" class="a">2018). The recent work of Shekhovtsov et al. </a>(2020) has shown that for stochastic binary networks the STE arises from particular linearization approximations.</p></li><li data-list-text="3.1.3"><p class="s59" style="padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark33">Bayesian Approaches for Quantization</a><a name="bookmark49">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark26" class="a">In this section, we review some quantization approaches, most of which are closely related to the Bayesian variational inference framework (see Section </a><a href="#bookmark26">2.7).</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark89" class="a">The work of Achterhold et al. </a><a href="#bookmark151" class="a">(2018) builds on the variational dropout based pruning approach of Louizos et al. </a><a href="#bookmark55" class="a">(2017) (see Section </a>3.2.3). They introduce a mixture of log-uniform priors whose mixtures are centered at predefined quantization values. Consequently, the approximate posterior also concentrates at these values such that weights can be safely quantized without requiring a fine-tuning procedure.</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">n</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;">The following works in this section directly operate on <i>discrete </i><a href="#bookmark178" class="a">weight distributions and, consequently, do not require a rounding procedure. Soudry et al. </a>(2014) approximate the true posterior <i>p</i>(<b>W </b><span class="s9">|D</span><a href="#bookmark156" class="a">) over discrete weights using expectation propagation (Minka, </a>2001) with closed-form online updates. Starting with an uninformative approximation <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>), their approach combines the current approximation <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i><a href="#bookmark27" class="a">) (serving as the prior in Bayes’ rule </a>(10)) with the likelihood for a single-sample dataset <span class="s9">D</span><i>n</i><i> </i>= <span class="s9">{</span>(<b>x</b><span class="s5">0</span> <i>, t</i><i>n</i>)<span class="s9">} </span>to obtain a refined posterior. To obtain a closed-form refinement step, they propose several approximations.</p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 16pt;text-align: justify;"><a href="#bookmark175" class="a">Although deviating from the Bayesian variational inference framework as no similarity measure to the true posterior is optimized, the approach of Shayer et al. </a>(2018) trains a distribution <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) over either binary weights <i>w </i><span class="s9">∈ {−</span>1<i>, </i>1<span class="s9">} </span>or ternary weights <i>w </i><span class="s9">∈</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 13pt;text-align: justify;"><span class="s9">{−</span>1<i>, </i>0<i>, </i>1<span class="s9">}</span>. They propose to minimize an expected loss E<span class="s19">q</span><span class="s61">(</span><span class="s62">W </span><span class="s63">|</span><span class="s19">ν</span><span class="s61">)</span><span class="s64">[</span><span class="s9">L</span>(<b>W</b>; <span class="s9">D</span>)] for the variational</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: justify;">parameters <i>ν </i>with gradient-based optimization using the local reparameterization trick</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark132" class="a">(Kingma et al., </a>2015). After training has finished, the discrete weights are obtained by either sampling or taking a mode from <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i><a href="#bookmark164" class="a">). Since their approach is limited to the ReLU activation function, Peters and Welling </a>(2018) extended their work to the sign activation function. This involves several non-trivial changes since the sign activation, due to its zero derivative, requires that the local reparameterization trick must be performed <i>after </i>the sign function. Consequently, <i>distributions </i><a href="#bookmark171" class="a">need to be propagated through commonly used building blocks such as batch normalization and pooling operations. Roth et al. </a>(2019) further extended these works to beyond three distinct discrete weights, and they introduced some technical improvements.</p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark188" class="a">Van Baalen et al. </a>(2020) propose a Bayesian mixed-precision quantization method for power-of-two bit widths. Their method is based on a recursive view of quantization where residual quantization errors are repeatedly quantized. They introduce gates that determine how many recursive quantization steps should be performed which in turn determines the number of used bits. While the quantization itself is subject to the STE, they propose to train gate probabilities using the Bayesian variational inference framework. The use of fewer bits for quantization is encouraged using a specific prior and, through an additional zero-bit gate, their framework simultaneously allows for weight pruning.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark115" class="a">Havasi et al. </a>(2019) introduced a novel Bayesian compression technique that we present here in this section although it is rather a coding technique than a quantization technique. In a nutshell, their approach first computes a variational distribution <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) over real-valued weights using mean field variational inference and then it encodes a sample from <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) in a smart way. They construct an approximation <i>q</i>˜(<b>W</b>) to <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) by importance</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">sampling using the prior <i>p</i>(<b>W</b>) as</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s30" style="text-indent: 0pt;line-height: 11pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s70" style="padding-top: 3pt;text-indent: 0pt;line-height: 6pt;text-align: right;">2<span class="s36">K</span></p><p style="padding-left: 61pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>) <span class="s9">≈ </span><i>q</i>˜(<b>W</b>) =  <u><i>q</i></u><u>(</u><u><b>W</b></u><span class="s72">i </span><span class="s73">|</span><span class="s74"> </span><u><i>ν</i></u><u>)</u><span class="s57"> </span><i>δ</i></p><p style="text-indent: 0pt;line-height: 7pt;text-align: right;"><i>p</i>(<b>W</b><i>i</i>) <span class="s75">W</span><span class="s76">i</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;">(<b>W</b>)  with <b>W</b><i>i</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a name="bookmark50"><span class="s9">~ </span></a><i>p</i>(<b>W</b>)<i>, </i>(13)</p><p class="s11" style="padding-left: 4pt;text-indent: 0pt;line-height: 8pt;text-align: center;">i<span class="s28">=1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">where <i>δ</i><span class="s46">W</span><span class="s77">i </span>denotes a point mass located at <b>W</b><i>i</i>. In the next step, a sample <b>W </b>from <i>q</i>˜(<b>W</b>) (or, equivalently, an approximate sample from <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i>)) is drawn which can be encoded by the corresponding number <i>k </i><span class="s9">∈ {</span>1<i>, . . . , </i>2<i>K</i><span class="s9">} </span>using <i>K </i><a href="#bookmark50" class="a">bits. Using the same random number generator initialized with the same seed as in </a>(13), the weights <b>W </b>can be recovered by sampling 2<i>K</i><i> </i>weights <b>W</b><i>i</i><i> </i>from the prior <i>p</i>(<b>W</b>) and selecting <b>W</b><i>k</i>. Since the number of samples 2<i>K</i><i> </i>required to obtain a reasonable approximation to <i>q</i>(<b>W </b><span class="s9">| </span><i>ν</i><a href="#bookmark50" class="a">) in </a>(13) grows exponentially with the number of weights, this sampling based compression scheme is performed for smaller weight blocks such that each weight block can be encoded with <i>K </i>bits.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="3.2"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark34">Network Pruning</a><a name="bookmark51">&zwnj;</a></h3><p style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Network pruning methods aim to achieve parameter sparsity by setting a substantial number of DNN weights to zero. Subsequently, the sparsity is exploited to improve resource efficiency of the DNN. On the one hand, there exist <i>unstructured </i>pruning approaches that set individual weights, regardless of their location in a weight tensor, to zero. Unstructured pruning approaches are typically less sensitive to accuracy degradation, but they require special sparse tensor data structures that in turn yield practical efficiency improvements only for very high sparsity. On the other hand, <i>structured </i>pruning methods aim to set whole weight structures to zero, e.g., by setting <i>all </i>weights of a matrix column to zero we would effectively prune an entire neuron. Conceptually, structured pruning is equivalent to removing tensor dimensions such that the reduced tensor remains compatible with highly optimized dense tensor operations.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">In this section, we start with the unstructured case which includes many of the earlier approaches and continue with structured pruning that has been the focus of more recent works. Then we review approaches that relate to Bayesian principles before we discuss approaches that prune structures dynamically during forward propagation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l9"><li data-list-text="3.2.1"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark35">Unstructured Pruning</a><a name="bookmark52">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">One of the earliest approaches to reduce the network size is the <i>optimal brain damage </i><a href="#bookmark136" class="a">algorithm of LeCun et al. </a><a href="#bookmark114" class="a">(1989). Their main finding is that pruning based on weight magnitude is suboptimal, and they propose a pruning scheme based on the increase in loss function. Assuming a pre-trained network, a local second-order Taylor expansion with a diagonal Hessian approximation is employed that allows us to estimate the change in loss function caused by weight pruning without re-evaluating the costly network function. Removing parameters is alternated with retraining the pruned network. In this way, the model size can be reduced substantially without deteriorating its performance. Hassibi and Stork </a>(1992) found the diagonal Hessian approximation to be too restrictive, and their <i>optimal brain surgeon </i><a href="#bookmark136" class="a">algorithm uses an approximated full covariance matrix instead. While their method, similar as in (LeCun et al., </a>1989), prunes weights that cause the least increase in loss function, the remaining weights are simultaneously adapted to compensate for the</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">negative effect of weight pruning. This bypasses the need to alternate several times between pruning and retraining the pruned network.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark112" class="a">However, it is not clear whether these approaches scale up to modern DNN architectures since computing the required (diagonal) Hessians is substantially more demanding (if not intractable) for millions of weights. Therefore, many of the more recently proposed techniques still resort to magnitude-based pruning. Han et al. </a>(2015) alternate between pruning connections below a certain magnitude threshold and re-training the pruned DNN. The results of this simple strategy are impressive, as the number of parameters in pruned</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 91%;text-align: justify;">DNNs is an order of magnitude smaller (9<span class="s9">× </span>for AlexNet and 13<span class="s9">× </span>for VGG-16) than in the original networks. Hence, this work shows that DNNs are often heavily over-parameterized.</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark113" class="a">In a follow-up paper, Han et al. </a>(2016) proposed <i>deep compression</i><a href="#bookmark112" class="a">, which extends the work in (Han et al., </a>2015) by a parameter quantization and parameter sharing step, followed by Huffman coding to exploit the non-uniform weight distribution. This approach yields a reduction in memory footprint by a factor of 35–49 and, consequently, a reduction in energy consumption by a factor of 3–5.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark110" class="a">Guo et al. </a>(2016) discovered that irreversible pruning decisions limit the achievable sparsity and that it is useful to reincorporate weights pruned in an earlier stage. In addition</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 83%;text-align: justify;">to each dense weight matrix <b>W </b><span class="s9">∈ </span>R<i>m</i><span class="s8">×</span><i>n</i>, they maintain a corresponding binary mask matrix <b>T </b><span class="s9">∈ {</span>0<i>, </i>1<span class="s9">}</span><i>m</i><span class="s8">×</span><i>n</i><i> </i>that determines whether a weight is currently pruned or not. In particular, the actual weights used during forward propagation are obtained as <b>W </b><span class="s9">⊙ </span><b>T </b>where <span class="s9">⊙ </span>denotes element-wise multiplication. Their method alternates between updating the weights <b>W </b>based on gradient descent, and updating the weight masks <b>T </b>by thresholding</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">the real-valued weights according to</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i,j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s45" style="padding-top: 2pt;text-indent: 0pt;line-height: 26pt;text-align: right;"><span class="s30"></span><span class="s78"></span><span class="p">0  if </span><span class="s9">|</span><span class="s6">W</span><span class="s7">t</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 3pt;text-indent: 0pt;text-align: left;"><span class="s9">| ∈ </span>[0<i>, a</i>)</p><p class="s6" style="text-indent: 0pt;line-height: 11pt;text-align: left;">T  <span class="p">=</span></p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 151pt;text-indent: 0pt;line-height: 7pt;text-align: left;">t<span class="s28">+1</span></p><p class="s11" style="padding-left: 150pt;text-indent: 0pt;line-height: 2pt;text-align: left;">i,j</p><p class="s11" style="padding-left: 30pt;text-indent: 0pt;line-height: 8pt;text-align: left;">t</p><p class="s6" style="text-indent: 0pt;line-height: 11pt;text-align: left;">T</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="padding-left: 29pt;text-indent: 0pt;line-height: 2pt;text-align: left;">i,j</p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 10pt;text-align: left;">if <span class="s9">|</span><i>W</i><i>t</i></p><p style="padding-left: 3pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><span class="s9">| ∈ </span>[<i>a, b</i>)</p><p class="s6" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark53">, </a><span class="p">(14)</span></p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i,j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i,j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-left: 37pt;text-indent: 0pt;line-height: 23pt;text-align: center;"><span class="s79"></span><span class="s80"></span><span class="p">1  if </span><span class="s9">|</span>W<span class="s7">t</span> <span class="s9">| ∈ </span><span class="p">[</span>b, <span class="s9">∞</span><span class="p">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">where <i>a </i>and <i>b </i>are two thresholds and <i>t </i>refers to the iteration number. Most importantly, weight updates are also applied to the currently pruned weights according to <b>T </b><a href="#bookmark53" class="a">using the STE, such that pruned weights can reappear in </a>(14). This reduces the number of parameters of AlexNet by a factor of 17.7 without deteriorating performance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.2.2"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark36">Structured Pruning</a><a name="bookmark54">&zwnj;</a></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: right;"><a href="#bookmark155" class="a">In (Mariet and Sra, </a>2016), a determinantal point process (DPP) is used to find a group of neurons that are diverse and exhibit little redundancy. Conceptually, a DPP for a given ground set <span class="s9">S </span>defines a distribution over subsets <i>S </i><span class="s9">⊆ S </span>where subsets containing diverse elements have high probability. They consider <span class="s9">S </span>to be the set of <i>N </i>-dimensional vectors that individual neurons compute over the whole dataset. Their approach samples a diverse set of neurons <i>S </i><span class="s9">⊆ S </span>according to the DPP and then prunes the other neurons <span class="s9">S \ </span><i>S</i><a href="#bookmark191" class="a">. To compensate for the negative effect of pruning, the outgoing weights of the remaining neurons after pruning are adapted so as to minimize the activation change of the next layer. Wen et al. </a><a href="#bookmark149" class="a">(2016) incorporated group lasso regularizers in the objective to obtain different kinds of sparsity in the course of training. They were able to remove filters, channels, and even entire layers in architectures containing shortcut connections. Liu et al. </a><a href="#bookmark149">(2017)</a></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">proposed to introduce an <i>l</i><span class="s5">1</span>-norm regularizer on the scale parameters <i>γ </i>of batch normalization and to set <i>γ </i>= 0 by thresholding. Since each batch normalization parameter <i>γ </i><a href="#bookmark123" class="a">corresponds to a particular channel in the network, this results in channel pruning with minimal changes to existing training pipelines. In (Huang and Wang, </a>2018), the outputs of different structures are scaled with individual trainable scaling factors. By using a sparsity enforcing <i>l</i><span class="s5">1</span>-norm regularizer on these scaling factors, the outputs of the corresponding structures are driven to zero and can be pruned.</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark154" class="a">Rather than pruning based on small parameter values, ThiNet (Luo et al., </a>2017) is a data-driven approach that prunes channels having the least impact on the subsequent layer.</p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">d,w,h</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">To prune channels in layer <i>l</i>, they propose to sample several activations <i>x</i><i>l</i><span class="s5">+1</span></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;">at randomly</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">selected spatial locations (<i>w, h</i>) and channels <i>d </i>of the following layer, and to greedily prune channels whose removal results in the least increase of squared error over these randomly selected activations. After pruning, they adapt the remaining filters to minimize the squared reconstruction error by minimizing a least squares problem.</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark152" class="a">Louizos et al. </a>(2018) propose to multiply weights with stochastic binary 0-1 gates associated with trainable probability parameters that effectively determine whether a weight should be pruned or not. They formulate an expected loss with respect to the distribution over the stochastic binary gates. By incorporating an expected <i>l</i><span class="s5">0</span><a href="#bookmark129" class="a">-norm regularizer over the weights, the probability parameters associated with these gates are encouraged to be close to zero. To enable the use of the reparameterization trick, a continuous relaxation of the binary gates using a modified binary Gumbel-softmax distribution is used (Jang et al., </a><a href="#bookmark140" class="a">2017). They show that their approach can be used for structured sparsity by associating the stochastic gates to entire structures such as channels. Li and Ji </a><a href="#bookmark196" class="a">(2019) extended this work by using the recently proposed unbiased ARM gradient estimator (Yin and Zhou, </a>2019) instead of using the biased Gumbel-softmax approximation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.2.3"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark37">Bayesian Pruning</a><a name="bookmark55">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s6" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 93%;text-align: justify;"><a href="#bookmark108" class="a">In (Graves, </a><a href="#bookmark93" class="a">2011; Blundell et al., </a><span class="p">2015), mean field variational inference is employed to obtain a factorized Gaussian approximation </span>q<span class="p">(</span><b>W </b><span class="s9">| </span>ν<span class="p">), i.e., instead of learning a deterministic weight </span>w <span class="p">per connection, they train for each connection a weight mean </span>w<span class="s12">µ</span> <span class="p">and a weight variance </span>w<span class="s81">σ</span><span class="s82">2 </span><span class="p">. After training, weights are pruned by thresholding the “signal-to-noise ratio” </span><span class="s9">|</span>w<span class="s12">µ</span>/w<span class="s12">σ</span><span class="s9">|</span><span class="p">.</span></p><p class="s6" style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark132" class="a">Some pruning approaches are based on variational dropout (Kingma et al., </a><span class="p">2015) which interprets dropout as performing variational inference with specific prior and approximate posterior distributions. Within this framework, the otherwise fixed dropout rates </span>α <a href="#bookmark159" class="a">of Gaussian dropout appear as free parameters that can be optimized to improve a variational lower bound. Molchanov et al. </a><span class="p">(2017) exploited this freedom to optimize individual weight dropout rates </span>w<span class="s12">α</span> <span class="p">such that weights </span>w <span class="p">can be safely pruned if their dropout rate </span>w<span class="s12">α</span> <a href="#bookmark151" class="a">is large. This idea has been extended by Louizos et al. </a><a href="#bookmark109" class="a">(2017) by using sparsity enforcing priors and assigning dropout rates to groups of weights that are all connected to the same structure, which in turn allows for structured pruning. Furthermore, they show how their approach can be used to determine an appropriate bit width for each weight by exploiting the well-known connection between Bayesian inference and the minimum description length (MDL) principle (Gru¨nwald, </a><a href="#bookmark109">2007).</a></p></li><li data-list-text="3.2.4"><p class="s59" style="padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark38">Dynamic Network Pruning</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">So far, we have presented methods that result in a fixed reduced architecture. In the following, we present methods that determine dynamically in the course of forward propagation which structures should be computed or, equivalently, which structures should be pruned. The intuition behind this idea is to vary the time spent for computing predictions based on the difficulty of the given input samples <b>x</b><span class="s5">0</span>.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark142" class="a">Lin et al. </a>(2017a) proposed to train, in addition to the DNN, a recurrent neural network (RNN) as decision network which determines the channels to be computed using reinforcement learning. In each layer, the feature maps are compressed using global pooling and fed into the RNN which aggregates state information over the layers to compute its pruning decisions.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark104" class="a">In (Dong et al., </a>2017), convolutional layers of a DNN are extended by a parallel low-cost convolution whose output after the ReLU function is used to scale the outputs of the potentially high-cost convolution. Due to the ReLU function, several outputs of the low-cost convolution will be exactly zero such that the computation of the corresponding output of the high-cost convolution can be omitted. For the low-cost convolution, they propose to</p><p class="s8" style="padding-left: 6pt;text-indent: 0pt;line-height: 85%;text-align: justify;"><span class="p">use weight tensors </span><span class="h3">W </span><span class="s9">∈ </span><span class="p">R</span><span class="s5">1</span>×<span class="s5">1</span>×<span class="s7">C</span>×<span class="s7">D</span><span class="s6"> </span><span class="p">and </span><span class="h3">W </span><span class="s9">∈ </span><span class="p">R</span><span class="s7">K</span>×<span class="s7">K</span>×<span class="s7">C</span>×<span class="s5">1</span><span class="p">. However, practical speed-ups are only reported for the </span><span class="s6">K </span><span class="s9">× </span><span class="s6">K </span><span class="p">convolution where all channels at a given spatial location might get set to zero.</span></p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark107" class="a">In a similar approach proposed by Gao et al. </a>(2019), the spatial dimensions of a feature map are reduced by global average pooling to a vector <b>a </b><span class="s9">∈ </span>R<i>C</i><i> </i>which is linearly transformed to <b>b </b><span class="s9">∈ </span>R<i>D</i><i> </i>using a single low-cost fully connected layer. To obtain a sparse vector <b>c </b><span class="s9">∈ </span>R<i>D</i>, <b>b </b>is fed into the ReLU function, followed by a <i>k</i>-winner-takes-all function that sets all entries of a vector to zero that are not among the <i>k </i>largest entries in absolute value. By multiplying <b>c </b>in a channel-wise manner to the output of a high-cost convolution, at least <i>D </i><span class="s9">− </span><i>k </i>channels will be zero and need not be computed. The number of channels <i>k </i>is derived from a predefined minimal pruning ratio hyperparameter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="3.3"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark39">Structural efficiency in DNNs</a></h3><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">In this section, we review strategies that establish certain structural properties in DNNs to improve computational efficiency. Each of the proposed subcategories in this section follows rather different principles and the individual techniques might not be mutually exclusive.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li data-list-text="3.3.1"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark40">Weight Sharing</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark46" class="a">Another technique to reduce the model size is weight sharing. We note that weight sharing and quantization methods (see Section </a>3.1) are closely related: Quantization methods often have an inherent weight sharing property since the number of possible quantization values is often much smaller than the number of weights. However, the purpose of a method is typically different depending on which category it belongs to. On the one hand, the focus of weight quantization methods typically lies on the employed numerical formats. The purpose of these formats is to reduce the storage per weight and to facilitate more efficient computations. Furthermore, the number of distinct weight values is typically rather small and fixed, and the particular weight values are often constrained or even fixed in advance. On the other hand, the purpose of weight sharing is to reduce the memory by reducing the</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark113" class="a">overall number of distinct weight values. For these methods, the particular weight values typically remain unconstrained. Note that some methods cannot be clearly attributed to either category, e.g., in deep compression (Han et al., </a>2016) weight sharing and quantization are in part used synonymously.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark97" class="a">In (Chen et al., </a>2015), a hashing function is used to randomly group network connections into “buckets”, where the connections in each bucket share the same weight value. The advantage of their approach is that weight assignments need not be stored explicitly since they are given implicitly by the hashing function. The authors show a memory footprint reduction by a factor of 10 while keeping the predictive performance essentially unaffected.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark186" class="a">Ullrich et al. </a><a href="#bookmark163" class="a">(2017) extended the soft weight sharing approach proposed by Nowlan and Hinton </a>(1992) to achieve both weight sharing and sparsity. The idea is to select a Gaussian mixture model prior over the weights and to train both the weights as well as the parameters of the mixture components. During training, the mixture components collapse to point measures and each weight gets attracted by a certain weight component. After training, weight sharing is obtained by assigning each weight to the mean of the component that best explains it, and weight pruning is obtained by assigning a relatively high mixture mass to a component with a fixed mean at zero.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark170" class="a">Roth and Pernkopf </a><a href="#bookmark160" class="a">(2018) utilized weight sharing to reduce the memory footprint of a large Bayesian ensemble of DNNs. The weight sharing is enforced by introducing a Dirichlet process prior over the weight prior distribution. They propose a sampling based inference scheme by alternately sampling weight assignments using Gibbs sampling and sampling weights using Hamiltonian Monte Carlo (Neal, </a>1992). By using the same weight assignments for multiple weight samples, the memory overhead for the weight assignments becomes negligible and the total memory footprint of an ensemble is reduced.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.3.2"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark41">Knowledge Distillation</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Knowledge distillation is a method where the knowledge contained in a large <i>teacher </i>model is transferred to a smaller <i>student </i>model. In the first step, a large teacher model is obtained with conventional training methods on the given training data. Subsequently, the smaller student model is trained on data where the ground truth labels have been replaced by the soft labels obtained from the output of the teacher model, e.g., from the softmax output of a DNN. It has been shown that this substantially increases the accuracy of the student model compared to directly training on the given training data.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">This general scheme is model agnostic, and early works applied knowledge distillation to compress <i>ensembles </i><a href="#bookmark198" class="a">of shallow neural networks (Zeng and Martinez, </a><a href="#bookmark94" class="a">2000) and other types of classifiers (Bucila et al., </a><a href="#bookmark198" class="a">2006) into a single neural network. Zeng and Martinez </a><a href="#bookmark94" class="a">(2000) have shown that training on soft labels obtained from the teacher results in higher accuracy than training on the actual hard predictions. The work of Bucila et al. </a>(2006) emphasizes the ability to train the student on unlabeled data to further reduce the accuracy gap between student and teacher. In addition, they presented a method to generate new synthetic inputs from the given training set, which might be useful if additional unlabeled data is limited or not available. They showed that the accuracy of the student can improve substantially when trained on these synthetically generated inputs.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark91" class="a">Ba and Caruana </a>(2014) applied these ideas to investigate the importance of depth in a DNN. They trained shallower (but not necessarily smaller) neural networks by mimicking the output activations <b>a</b><i>L</i><i> </i>produced by a teacher DNN before applying the softmax function. The resulting shallow models perform similar as their deeper counterparts which was not achievable by training the shallow model on the ground truth targets directly. Therefore, the authors conclude that shallower models are as expressive as deeper models but they are more difficult to train.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark139" class="a">The work of Li et al. </a><a href="#bookmark118" class="a">(2014) and Hinton et al. (2015) applied knowledge distillation with the main focus on reducing model complexity of a large teacher DNN. Hinton et al. </a>(2015) proposed to obtain the soft labels <b>y</b>ˆ from the teacher by scaling the output activations with a temperature <i>τ &gt; </i>0 as</p><p class="s83" style="text-indent: 0pt;line-height: 8pt;text-align: left;">i &nbsp; </p><p style="text-indent: 0pt;text-align: left;"/><p class="s30" style="text-indent: 0pt;line-height: 11pt;text-align: left;">Σ</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><span class="s84"> </span><span class="p">exp(</span>a<span class="s7">L</span>/τ <span class="p">)</span></p><p class="s11" style="text-indent: 0pt;line-height: 8pt;text-align: left;">j</p><p style="text-indent: 0pt;text-align: left;"/><p class="s6" style="text-indent: 0pt;line-height: 11pt;text-align: right;">y<span class="p">ˆ</span><span class="s12">i</span> <span class="p">=</span></p><p class="s11" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">j</p><p class="s6" style="text-indent: 0pt;line-height: 19pt;text-align: left;"><a name="bookmark56"><span class="p">exp(</span></a>a<span class="s7">L</span>/τ <span class="p">) </span><span class="s85">. </span><span class="s86">(15)</span></p><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">For <i>τ &gt; </i>1, the labels tend to become more uniform which has been reported to facilitate training. Furthermore, they propose to utilize the ground truth labels by minimizing a weighted average of the traditional cross-entropy loss based on the ground truth labels <i>t </i>and the knowledge distillation loss based on the soft targets <b>y</b><a href="#bookmark56" class="a">ˆ in </a><a href="#bookmark118" class="a">(15). Noteworthy, it was the work of Hinton et al. </a>(2015) that coined the term <i>knowledge distillation</i>.</p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark169" class="a">FitNets (Romero et al., </a>2015) extend these ideas by also transferring knowledge from intermediate layers. They select an intermediate layer from the teacher DNN as the <i>hint layer </i>which they try to mimic in an intermediate <i>guide layer </i><a href="#bookmark118" class="a">of the student DNN. Since the hint layer and the guide layer are generally of different size, they introduce a regressor that predicts the hint layer from the guide layer. This ensures that the guide layer contains the same information as the hint layer. The proposed procedure operates in two stages. In the first stage, the student is trained up to the guide layer by minimizing the discrepancy between guide and hint layer. In the second stage, the whole student DNN is trained using conventional knowledge distillation as in (Hinton et al., </a><a href="#bookmark118">2015).</a></p><p style="padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: right;"><a href="#bookmark130" class="a">Kim et al. </a><a href="#bookmark169" class="a">(2018) argue that matching the raw features of certain intermediate layers as in (Romero et al., </a>2015) is suboptimal since it is difficult to compare individual layers of different DNNs. Therefore, they propose a method to match more understandable <i>factors </i>extracted from the intermediate layers of the student and the teacher DNNs. Starting from a pre-trained teacher DNN, they first train an autoencoder which they call <i>paraphraser </i>to extract understandable factors from a selected intermediate layer of the teacher DNN. The student DNN is extended by a regressor which they call <i>translator </i><a href="#bookmark157" class="a">whose purpose is to predict the paraphraser factors from the features of a selected intermediate layer. The student DNN is then trained to simultaneously minimize the cross-entropy loss on the ground truth labels and the difference between paraphraser and translator output. They employ the paraphraser and the translator after the last convolutional layer in their DNNs. In the context of quantization, knowledge distillation has been used to reduce the accuracy gap between real-valued DNNs and quantized DNNs (Mishra and Marr, </a><a href="#bookmark166" class="a">2018; Polino et al., </a><a href="#bookmark157" class="a">2018). In particular, a real-valued teacher DNN is used to improve the accuracy of a quantized student DNN. Mishra and Marr </a>(2018) showed improved results using three different modes of knowledge distillation training, including a mode where the student and</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">the teacher are trained simultaneously from scratch.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark165" class="a">Phuong and Lampert </a>(2019) transferred knowledge between different parts of the <i>same </i>model. They employ multi-exit architectures which provide anytime predictions after certain intermediate layers; therefore, allowing for a trade-off between accuracy and prediction latency at run-time. The knowledge from the (most accurate) final layer is transferred to the earlier exits to improve their accuracy. Furthermore, they show that the earlier layers can be trained with unlabeled data in a semi-supervised setting.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark133" class="a">In a Bayesian context, Korattikara et al. </a>(2015) applied knowledge distillation to condense a large ensemble of DNNs, for instance, obtained by sampling from the posterior distribution <i>p</i>(<b>W </b><span class="s9">|D</span><a href="#bookmark28" class="a">). In this way, expected predictions </a><a href="#bookmark190" class="a">(11) obtained by averaging the outputs of the individual models can be transferred to a single DNN. Their method trains a single student DNN using the outputs of teacher DNNs that are generated on the fly using SGLD (Welling and Teh, </a><a href="#bookmark190">2011).</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.3.3"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark42">Special Matrix Structures</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">In this section, we review approaches that aim at reducing the model size by employing efficient matrix representations. There exist several methods using low-rank decompositions which represent a large matrix (or a large tensor) using only a fraction of the parameters. In most cases, the implicitly represented matrix is never computed explicitly such that also a computational speed-up is achieved. Furthermore, there exist approaches using special matrices that are specified by only few parameters and whose structure allows for extremely efficient matrix multiplications.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark101" class="a">Denil et al. </a>(2013) proposed a method that is motivated by training only a subset of the weights and predicting the values of the other weights from this subset. In particular, they represent weight matrices <b>W </b><span class="s9">∈ </span>R<i>m</i><span class="s8">×</span><i>n</i><i> </i>using a low-rank approximation <b>UV </b>with <b>U </b><span class="s9">∈ </span>R<i>m</i><span class="s8">×</span><i>k</i>, <b>V </b><span class="s9">∈ </span>R<i>k</i><span class="s8">×</span><i>n</i>, and <i>k &lt; </i>min<span class="s9">{</span><i>m, n</i><span class="s9">} </span>to reduce the number of parameters. Instead of learning both factors <b>U </b>and <b>V</b>, prior knowledge, such as smoothness of pixel intensities in an image, is incorporated to compute a fixed <b>V </b>using kernel-techniques or auto-encoders, and only the factor <b>U </b>is learned.</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark162" class="a">In (Novikov et al., </a>2015), the tensor train matrix format is employed to substantially reduce the number of parameters required to represent large weight matrices of fully connected layers. Their approach enables the training of very large fully connected layers with relatively few parameters, and they achieve improved performance compared to simple low-rank approximations.</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark103" class="a">Denton et al. </a><a href="#bookmark128" class="a">(2014) propose specific low-rank approximations and clustering techniques for individual layers of pre-trained CNNs to reduce both memory footprint and computational overhead. Their approach yields substantial improvements for both the computational bottleneck in the convolutional layers and the memory bottleneck in the fully connected layers. By fine-tuning after applying their approximations, the performance degradation is kept at a decent level. Jaderberg et al. </a><a href="#bookmark135" class="a">(2014) propose two different methods to approximate pre-trained CNN filters as combinations of rank-1 basis filters to speed up computation. The rank-1 basis filters are obtained either by minimizing a reconstruction error of the original filters or by minimizing a reconstruction error of the outputs of the convolutional layers. Lebedev et al. </a>(2015) approximate the convolution tensor using the canonical polyadic (CP) decomposition—a generalization of low-rank matrix decomposi-</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">tions to tensors—using non-linear least squares. Subsequently, the convolution using this low-rank approximation is performed by four consecutive convolutions, each with a smaller filter, to reduce the computation time substantially.</p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;text-align: justify;"><a href="#bookmark98" class="a">In (Cheng et al., </a>2015), the weight matrices of fully connected layers are restricted to circulant matrices <b>W </b><span class="s9">∈ </span>R<i>n</i><span class="s8">×</span><i>n</i>, which are fully specified by only <i>n </i><a href="#bookmark195" class="a">parameters. While this dramatically reduces the memory footprint of fully connected layers, circulant matrices also facilitate faster computation as matrix-vector multiplication can be efficiently computed using the fast Fourier transform. In a similar vein, Yang et al. </a>(2015) reparameterize matrices <b>W </b><span class="s9">∈ </span>R<i>n</i><span class="s8">×</span><i>n</i><i> </i>of fully connected layers using the Fastfood transform as <b>W</b>=<b>SHGΠHB</b>, where <b>S</b>, <b>G</b>, and <b>B </b>are diagonal matrices, <b>Π </b>is a random permutation matrix, and <b>H </b>is the Walsh-Hadamard matrix. This reparameterization requires only a total of 4<i>n </i><a href="#bookmark98" class="a">parameters, and similar as in (Cheng et al., </a>2015), the fast Hadamard transform enables an efficient computation of matrix-vector products.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="3.3.4"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark43">Manual Architecture Design</a><a name="bookmark57">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Instead of modifying existing architectures to make them more efficient, manual architecture design is concerned with the development of new architectures that are inherently resource-efficient. Over the past years, several design principles and building blocks for DNN architectures have emerged that exhibit favorable computational properties and sometimes also improve performance.</p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">CNN architectures are typically designed to have a transition from convolutional layers to fully connected layers. At this transition, activations at all spatial locations of each channel are typically used as individual input features for the following fully connected layer. Since the number of these features is typically large, there is a memory bottleneck for storing the parameters of the weight matrix especially in the first fully connected layer.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark143" class="a">Lin et al. </a>(2014a) introduced two concepts that have been widely adopted by subsequent works. The first one, <i>global average pooling</i>, largely solves the above-mentioned memory issue at the transition to fully connected layers. Global average pooling reduces the spatial dimensions of each channel into a single feature by averaging over all values within a channel. This reduces the number of features at the transition drastically, and, by having the same number of channels as there are classes, it can also be used to completely get rid of fully connected layers. Second, they used 1 <span class="s9">× </span>1 convolutions with weight kernels <b>W </b><span class="s9">∈ </span>R<span class="s5">1</span><span class="s8">×</span><span class="s5">1</span><span class="s8">×</span><i>C</i><span class="s8">×</span><i>D</i><i> </i>which can be seen as performing the operation of a fully connected layer over each spatial location across all channels.</p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;">These 1 <span class="s9">× </span><a href="#bookmark181" class="a">1 convolutions have been adopted by several popular architectures (Szegedy et al., </a><a href="#bookmark116" class="a">2015; He et al., </a><a href="#bookmark122" class="a">2016; Huang et al., </a><a href="#bookmark181" class="a">2017) and, due to their favorable computational properties compared to convolutions that take a spatial neighborhood into account, later have also been exploited to improve computational efficiency. For instance, InceptionNet (Szegedy et al., </a>2015) proposed to split standard <i>K </i><span class="s9">× </span><i>K </i>convolutions into two cheaper convolutions: (i) a 1 <span class="s9">× </span>1 convolution to reduce the number of channels such that (ii) a subsequent <i>K </i><span class="s9">× </span><i>K </i>convolution is performed faster. Similar ideas are used in <a href="#bookmark125" style=" color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt;">SqueezeNet </a><a href="#bookmark125" class="a">(Iandola et al., </a>2016) which employs 1 <span class="s9">× </span>1 convolutions to reduce the number of input channels of subsequent parallel 1 <span class="s9">× </span>1 and 3 <span class="s9">× </span>3 convolutions. In addition, SqueezeNet uses the global average pooling output of per-class channels directly as input to the softmax in</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark113" class="a">order to avoid fully connected layers that typically consume the most memory. On top of that, they also applied deep compression (Han et al., </a><a href="#bookmark52" class="a">2016) (see Section </a>3.2.1) to reduce the memory footprint of their model even further.</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 16pt;line-height: 88%;text-align: justify;"><a href="#bookmark182" class="a">Szegedy et al. </a>(2016) extended the InceptionNet architecture by spatially separable convolutions to reduce the computational complexity, i.e., a <i>K </i><span class="s9">× </span><i>K </i>convolution is split into a <i>K </i><span class="s9">× </span>1 convolution followed by a 1 <span class="s9">× </span><i>K </i>convolution. In <a href="#bookmark121" style=" color: black; font-family:Georgia, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt;">MobileNet </a><a href="#bookmark121" class="a">(Howard et al., </a>2017) depthwise separable convolutions are used to split a standard convolution in another way:</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">(i) a depthwise convolution and (ii) a 1 <span class="s9">× </span>1 convolution. The depthwise convolution applies a <i>K </i><span class="s9">× </span><i>K </i>filter to each channel separately without taking the other channels into account whereas the 1 <span class="s9">× </span>1 convolution then aggregates information across channels. Although these two cheaper convolutions together are less expressive than a standard convolution, they can be used to trade off a small loss in prediction accuracy with a drastic reduction in computational overhead and memory requirements.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark173" class="a">Sandler et al. </a>(2018) extended these ideas in their <i>MobileNetV2 </i><a href="#bookmark116" class="a">to an architecture with residual connections. A typical residual block with bottleneck structure in ResNet (He et al., </a>2016) contains a 1 <span class="s9">× </span>1 bottleneck convolution to reduce the number of channels, followed by a 3 <span class="s9">× </span>3 convolution, followed by another 1 <span class="s9">× </span>1 convolution to restore the original number of channels again. Contrary to that building block, MobileNetV2 introduces an <i>inverted </i>bottleneck structure where the shortcut path contains the bottleneck and the residual path performs computations in a high-dimensional space. In particular, the residual path performs a 1 <span class="s9">× </span>1 convolution to <i>increase </i>the number of channels, followed by a cheap <i>depthwise </i>3 <span class="s9">× </span>3 convolution, followed by another 1 <span class="s9">× </span>1 convolution to reduce the number of channels again. They show that their inverted structure is more memory efficient since the shortcut path, which needs to be kept in memory during computation of the residual path, is considerably smaller. Furthermore, they show improved performance compared to the standard bottleneck structure.</p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 16pt;text-align: justify;"><a href="#bookmark134" class="a">While it was more of a technical detail rather than a contribution on its own, AlexNet (Krizhevsky et al., </a>2012) used <i>grouped convolutions </i>with two groups to facilitate model parallelism for training on two GPUs with relatively low memory capacity. Instead of computing a convolution using a weight tensor <b>W </b><span class="s9">∈ </span>R<i>K</i><span class="s8">×</span><i>K</i><span class="s8">×</span><i>gC</i><span class="s8">×</span><i>gD</i>, a grouped convolution splits the input into <i>g </i>groups of <i>C </i>channels that are independently processed using weight tensors <b>W</b><i>g</i><i> </i><span class="s9">∈ </span>R<i>K</i><span class="s8">×</span><i>K</i><span class="s8">×</span><i>C</i><span class="s8">×</span><i>D</i>. The outputs of these <i>g </i>convolutions are then stacked again such that the same number of input and output channels are maintained while considerably reducing the computational overhead and memory footprint.</p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 14pt;text-align: justify;"><a href="#bookmark194" class="a">Although this reduces the expressiveness of the convolutional layer since there is no interaction between the different groups, Xie et al. </a><a href="#bookmark200" class="a">(2017) used grouped convolutions to enlarge the number of channels of a ResNet model which resulted in accuracy gains while keeping the computational complexity of the original ResNet model approximately the same. Zhang et al. </a>(2018b) introduced a ResNet-inspired architecture called <i>ShuffleNet </i>which employs 1 <span class="s9">× </span>1 <i>grouped </i>convolutions since 1 <span class="s9">× </span><a href="#bookmark121" class="a">1 convolutions have been identified as computational bottlenecks in previous works, e.g., see (Howard et al., </a>2017). To combine the computational efficiency of grouped convolutions with the expressiveness of a full convolution, ShuffleNet incorporates <i>channel shuffle </i>operations after grouped convolutions to partly recover the interaction between different groups.</p></li><li data-list-text="3.3.5"><p class="s59" style="padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark44">Neural Architecture Search</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Neural architecture search (NAS) is a recently emerging field concerned with the automatic discovery of good DNN architectures. This is achieved by designing a discrete space of possible architectures in which we subsequently search for an architecture that optimizes some objective – typically the validation error. By incorporating a measure of resource efficiency into this objective, this technique has recently attracted attention for the automatic discovery of resource-efficient architectures.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The task is very challenging: On the one hand, evaluating the validation error is time-consuming as it requires a full training run and typically only results in a noisy estimate thereof. On the other hand, the space of architectures is typically of exponential size in the number of layers. Hence, the space of architectures needs to be carefully designed in order to facilitate an efficient search within that space.</p><p style="padding-top: 4pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark204" class="a">The influential work of Zoph and Le </a><a href="#bookmark205" class="a">(2017) introduced a scheme to encode DNN architectures of arbitrary depth as sequences of tokens which can be sampled from a controller RNN. This controller RNN is trained with reinforcement learning to generate well performing architectures using the validation error on a held-out validation set as a reward signal. However, the training effort is enormous since more than 10,000 training runs are required to achieve state-of-the-art performance on CIFAR-10. This would be impractical on larger datasets such as ImageNet which was partly solved by subsequent NAS approaches, e.g., in (Zoph et al., </a>2018). In this review, we highlight methods that also consider resource efficiency constraints for NAS.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark184" class="a">In MnasNet (Tan et al., </a><a href="#bookmark144" class="a">2018), a RNN controller is trained by also considering the latency of the sampled DNN architecture measured on a real mobile device. They achieve performance improvements under predefined latency constraints on a specific device. To run MnasNet on the large-scale datasets ImageNet and COCO (Lin et al., </a>2014b), their algorithm is run on a <i>proxy task </i>by only training for five epochs, and only the most promising DNN architectures were trained using more epochs.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark189" class="a">Wang et al. </a>(2019) determined the individual bit widths of mixed-precision quantization using a similar reinforcement learning framework. Their controller DNN generates for each layer two bit widths, one for the weights and one for the activations. A pre-trained full-precision DNN is then quantized using these bit widths and fine-tuned for one epoch to obtain a reward signal that is subsequently used to update the controller. Their method incorporates hardware-specific constraints, such as latency and energy consumption, that must be met by the controller.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark95" class="a">Instead of generating architectures using a controller, ProxylessNAS (Cai et al., </a>2019) uses a heavily over-parameterized model where each layer contains several parallel paths, each computing a different architectural block with its individual parameters. For each layer, probability parameters for selecting a particular architectural block are introduced which are trained via backpropagation using the STE. After training, the most probable path determines the selected architecture. To favor resource-efficient architectures, a latency model is build using measurements done on a specific real device whose predicted latencies are used as a differentiable regularizer in the cost function. In their experiments, they show that different target devices prefer individual DNN architectures to obtain a low latency.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark180" class="a">Instead of using a different path for different operations in each layer, single-path NAS (Stamoulis et al., </a>2019) combines all operations in a single <i>shared weight superblock </i>such that each operation uses a subset of this superblock. A weight-magnitude-based decision using trainable threshold parameters determines which operation should be performed, allowing for gradient-based training of both the weight parameters and the architecture. Again, the STE is employed to backpropagate through the threshold function.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark192" class="a">Wu et al. </a><a href="#bookmark147" class="a">(2018a) performed mixed-precision quantization using similar NAS concepts to those used by Liu et al. </a><a href="#bookmark95" class="a">(2019a) and Cai et al. </a>(2019). They introduce gates for every layer that determine the number of bits used for quantization, and they perform continuous stochastic optimization of probability parameters associated with each of these gates.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark150" class="a">Liu et al. </a><a href="#bookmark51" class="a">(2019b) have replicated several experiments of pruning approaches (see Section </a>3.2) and they observed that the typical workflow of training, pruning, and fine-tuning is often not necessary and only the discovered sparsity structure is important. In particular, they show for several pruning approaches that randomly initializing the weights after pruning and training the pruned structure from scratch results in most cases in a similar performance as performing fine-tuning after pruning. They conclude that network pruning can also be seen as a paradigm for architecture search.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark183" class="a">Tan and Le </a>(2019) recently proposed EfficientNet which employs NAS for finding a resource-efficient architecture as a key component. In the first step, they perform NAS to discover a small resource-efficient model which is much cheaper than searching for a large model directly. In the next step, the discovered model is enlarged by a principled compound scaling approach which simultaneously increases the number of layers, the number of channels, and the spatial resolution. Although this approach is not targeting resource efficiency on its own, EfficientNet achieves state-of-the-art performance on ImageNet using a relatively small model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li></ol></li><li data-list-text="4."><h2 style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark58">Embedded Hardware for Deep Neural Networks</a><a name="bookmark64">&zwnj;</a></h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Improvements in hardware for deep learning are a key driver for the recent success stories of AI applications through DNNs. Both training and inference have extremely high demands on their targeted platform and certain hardware requirements can be the deciding factor whether an application can be realized. This section briefly introduces the most important hardware for deep learning and discusses their potentials and limitations. While this discussion is generic and independent from training or inference, it should be noted that all processor concepts are available in different scales, ranging from mobile to server variants.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l11"><li data-list-text="4.1"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark59">CPUs</a></h3><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark102" class="a">CPUs were originally designed to optimize single-thread performance in order to execute an individual computation within the shortest possible latency. Unfortunately, single-thread performance is stagnating since the end of Dennard scaling (Dennard et al., </a>1974), and now performance scaling usually requires parallelization. While multithreading is a rather obvious solution for parallelization that is applicable to many tasks, vectorization is a technique that promises great potential for certain applications. Vectorization applies a single instruction to multiple pre-selected data elements and, thus, avoids costly at-runtime dependency checking while maximizing instruction reuse. CPUs show excellent properties of exploiting</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">sparse DNNs due to their short vector units and the low amount of multithreading together with high frequency. Furthermore, they usually support 8-bit integer formats and feature certain instructions for extremely low representation and, consequently, they are well suited for quantization operations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="4.2"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark60">GPUs</a></h3><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">GPUs were initially designed to accelerate image and video processing only and are nowadays the most popular general-purpose accelerators for many tasks, such as scientific and AI computations. The architecture consists of many streaming multiprocessors which are highly parallel and each implements many lightweight cores. Thus, GPUs are massively parallel processors with large memory that provides extremely high bandwidth and throughput, but significantly lower frequency in comparison to CPUs. The extremely high amount of parallelism and the resulting demand on structured computations, however, virtually prevents the deployment of sparse computations. Modern GPU designs and their respective software stack implement support for reduced-precision computations, such as 8-bit integer and half-precision floating-point formats, which are very well suited for deep learning. More extreme forms of quantization are not yet supported and do not result in more efficient inference or training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="4.3"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark61">FPGAs</a></h3><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Field-programmable gate arrays (FPGAs) are a family of processors that implement a large array of configurable logic blocks which can be programmed using hardware description languages (e.g., VHDL, Verilog, HLS). This concept is the main difference to ASICs in terms of technology since the hardware can be designed for specific functional or application requirements. While this reconfigurability enables various opportunities that go beyond the capabilities of CPUs and GPUs, it comes at the cost of much lower frequency and reduced on-chip memory. FPGAs are in principle very well suited for DNNs, since compute units can be specifically tailored to fit the diverse computations while also enabling massive amounts of parallelism. Reconfigurable hardware is especially interesting for compressed DNNs due to their flexibility to implement any data format as well as sparse logic.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="4.4"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark62">Domain-Specific Accelerator</a></h3><p style="padding-top: 8pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">Recent interest in deep learning has motivated to push advancements in the development of custom accelerators, such as Google’s TPUs and Graphcore’s IPU. The key feature of the TPU (and most of the other deep learning accelerators) is a 256<span class="s9">×</span>256 matrix-multiplication unit that is referred to as <i>systolic array</i>. Systolic arrays are a variant of massively parallel processor arrays that are very suitable for regular tasks, such as linear algebra operations, and a promising candidate to address the increasing costs of data movements. The objective of such arrays is to minimize instruction fetch and data access costs by constraining the data flow to matrix and vector operations. However, data movements can only be reduced if locality effects are sufficiently exploited and the data flow constraints of a systolic array may result in poor utilization and latency increase. Such domain-specific accelerators are usually highly constrained when aiming to optimize DNNs through compression. For instance, the</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">TPU supports 8-bit integer and half-precision floating-point formats while other (potentially lower-precision) representations are not efficiently supported by hardware. Furthermore, the dense structure of the systolic arrays demands for similarly dense computations and cannot exploit fine-grained sparsity patterns.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="4.5"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark63">Loop-Back vs. Data-Flow Architectures</a></h3><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">One can roughly categorize hardware platforms for deep learning inference into loop-back and data-flow architectures. Loop-back architectures use a fixed processor and memory system to move data from off-chip memory to the processor and leverage the available compute resources. This is performed for each layer or operation sequentially until inference computation has finished. The drawback of loop-back architectures is that they potentially require many data movements from and to off-chip memory, which is time and energy consuming. CPUs aim to reduce these memory accesses by featuring large on-chip caches and reuse data as much as possible. Similar are domain-specific accelerators, such as TPUs, which usually feature a large and programmable scratch pad memory on chip. On the contrary, GPUs feature large register files and aim to hide memory latency by leveraging parallel slackness. Another critical aspect of loop-back architectures is low compute utilization, which can potentially occur if certain layer or operation types do not fit the static compute array (i.e., if operation size is too low). The advantage of such a <i>generic </i>compute architecture is that they allow arbitrary operations in combination with productive code generation since the hardware does not need to be optimized for a certain task. Continuous improvements in semi-conductor and processor technology are the main improvement factor of such inference engines.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">In contrast to this, data-flow architectures use a reconfigurable processor and memory system for computing the inference. Here, each layer or operation within a neural architecture is assigned a dedicated compute engine and its own memory subsystem in order to enable inference in a pipelined fashion. This avoids off-chip accesses for intermediate operations completely by simple forwarding the computed results to the next hardware layer. Furthermore, data-flow architectures achieve excellent utilization of the available hardware logic, since several compute engines can be tailored to the required operation type and latency. One drawback of this data-flow architecture is, however, that it requires long development costs because it does not only require software but also hardware optimizations. In addition, reconfigurable hardware comes at the cost of reduced absolute compute power in comparison to ASIC designs. The main limitation of data-flow architectures is that they demand the entire neural architecture (weights and activations) to be put on chip, which is highly restrictive for large models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="5."><h2 style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark65">Experimental Results</a><a name="bookmark74">&zwnj;</a></h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark46" class="a">We provide experimental results for modern DNN architectures trained on well-known benchmark datasets. The focus of our experiments is on quantization (see Section </a><a href="#bookmark54" class="a">3.1) and structured pruning approaches (see Section </a>3.2.2) since they are among the earliest and most efficient approaches to enhance the computational efficiency of DNNs.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark76" class="a">We compare several quantization approaches discussed in this paper in terms of prediction quality in Section </a>5.1.1. Next, we compare different DNN architectures and pruning</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark78" class="a">structures (i.e., the type of structure considered for pruning, such as channels) using model metrics such as number of computations and memory requirements in Section </a><a href="#bookmark80" class="a">5.1.2. Furthermore, we benchmark the compressed models on mobile variants of CPU (Section </a><a href="#bookmark82" class="a">5.2.1), FPGA (Section </a><a href="#bookmark84" class="a">5.2.2), and GPU (Section </a><a href="#bookmark86" class="a">5.2.3), and provide an overall comparison in Section </a><a href="#bookmark86">5.2.4.</a></p><p style="padding-top: 5pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark45" class="a">The main focus of this section is to showcase the difficulty of finding good trade-offs between resource efficiency and predictive performance. As this paper is mainly dedicated to giving a comprehensive literature overview of the current state of the art, an extensive evaluation of the many presented methods in Section </a>3 would be infeasible and it is also not within the scope of this paper.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li data-list-text="5.1"><h3 style="padding-top: 6pt;padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark66">Prediction Quality of Compressed DNNs</a><a name="bookmark75">&zwnj;</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l13"><li data-list-text="5.1.1"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark67">Prediction Quality using Different Quantization Approaches</a><a name="bookmark76">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;"><a href="#bookmark122" class="a">In the first experiment we compare the performance of several quantization approaches. We use a DenseNet architecture (Huang et al., </a>2017) consisting of 100 layers with bottleneck and compression layers, i.e., a DenseNet-BC-100. We select the default growth rate of <i>k </i>= 12 for the model, i.e., the number of feature maps added per layer. We conduct our experiments on the CIFAR-100 dataset where the task is to classify RGB images of size 32<span class="s9">×</span><a href="#bookmark46" class="a">32 pixels to one of 100 object categories. The CIFAR-100 dataset is split into 50,000 training images and 10,000 test images. We selected some of the most popular quantization approaches (see Section </a><a href="#bookmark100" class="a">3.1) for our comparison: binary weight networks (BWN) (Courbariaux et al., </a><a href="#bookmark124" class="a">2015b), binarized neural networks (BNN) (Hubara et al., </a><a href="#bookmark202" class="a">2016), DoReFa-Net (Zhou et al., </a><a href="#bookmark203" class="a">2016), trained ternary quantization (TTQ) (Zhu et al., </a><a href="#bookmark199" class="a">2017), and LQ-Net (Zhang et al., </a>2018a). For this experiment, we quantize the DNNs in three different modes: (i) weight-only, (ii) activation-only, and (iii) combined weight and activation quantization. However, note that some quantization approaches are designed for a particular mode, e.g., BWN and TTQ only consider weight quantization whereas BNN only considers combined weight and activation quantization.</p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark77" class="a">Figure </a>3 reports the test errors for different bit widths of the selected quantization approaches. The horizontal red line shows the test error of the real-valued baseline DenseNet-BC-100. For combined weight and activation quantization we use the same bit widths for the weights and the activations.</p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;"><a href="#bookmark46" class="a">As expected, the test error decreases gradually with increasing bit widths for all quantization modes and for all quantization approaches. Furthermore, the results indicate that prediction performance is more sensitive to activation quantization than to weight quantization, which is in line with the results reported by many works reviewed in Section </a><a href="#bookmark46">3.1.</a></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The more advanced LQ-Net approach clearly outperforms the rather simple linear quantization of DoReFa-Net and the specialized binary and ternary approaches. However, this performance improvement comes at the cost of longer training times. For instance, the training time per iteration increases—in relation to training without quantization—for DoReFa-Net by a factor of 1.5 compared to a factor of up to 4.6 (depending on the bit width) for LQ-Net.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="391" height="294" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark46" class="a" name="bookmark77">Figure 3: Comparison of several popular quantization approaches (see Section </a>3.1) using the DenseNet-BC-100 architecture trained on the CIFAR-100 dataset. The horizontal red line shows the error of the real-valued baseline. Quantization is performed using different bit widths in three different modes: activation-only (blue), weight-only (green), and combined weight and activation quantization (purple).</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="5.1.2"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark68">Prediction Quality using Different Pruning Structures</a><a name="bookmark78">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark197" class="a">In the next experiment, we explore the performance metrics of different DNN architectures (ResNet and DenseNet) and pruning structures (such as channels, kernels and groups) on the CIFAR-10 task. CIFAR-10 is similar to CIFAR-100 used in the previous section (i.e., image size and size of training and test sets are equal) except that it contains only ten object classes. We use wide residual networks (WRNs) by Zagoruyko and Komodakis </a><a href="#bookmark174" class="a">(2016) with a depth of 28 layers, one of the top performing architectures on this task. This architecture is identical to the original ResNet model except that it is scaled in width rather than depth. Additionally, we create a DenseNet variant for this experiment which is scaled in depth to 28 layers and the width is varied until it approximately matches the number of parameters and computations of the WRN model in order to guarantee a fair comparison. We apply parameterized structured pruning (PSP) by Schindler et al. </a><a href="#bookmark57" class="a">(2020), a method that allows to dynamically learn the shape of DNNs through structured sparsity. PSP parameterizes arbitrary structures in a weight tensor and leverages weight decay to detect unimportant structures that can be pruned. In this experiment, we select pruning structures that are in line with commonly used DNN libraries for convolutions: we use channel pruning to learn the number of input and output feature maps, kernel pruning to learn the size of the convolution kernel, and group pruning to learn heterogeneous group sizes for grouped convolutions (see Section </a>3.3.4 for a discussion on grouped convolutions).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="251" height="191" src="data:image/jpg;base64,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"/></td></tr></table></span>	<span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="251" height="191" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="251" height="191" src="data:image/jpg;base64,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"/></td></tr></table></span>	<span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="251" height="191" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: left;"><a name="bookmark79">Figure 4: Performance metrics of several pruning structures and DNN architectures on CIFAR-10 for WRNs and DenseNets.</a></p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">In a first step, channel (<i>WRN: Channel </i>), kernel (<i>WRN: Kernel </i>), and group pruning (<i>WRN: Group</i>) are evaluated separately on the WRN architecture. The results for number of floating-point operations (FLOPs), parameters, activations, and memory (= parameters</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark79" class="a">+ activations) are reported in Figure </a>4. When considering the number of FLOPs and parameters, which are the main metrics in the literature on resource-efficient DNNs, it is clear that channel and group pruning significantly outperform kernel pruning. This indicates a high sensitivity of the kernel size to the overall accuracy. Group and channel pruning perform very similar with respect to FLOPs and parameters, especially for highly compressed models. However, when also considering the number of activations and overall memory consumption (i.e., parameters and activations), group pruning performs significantly worse since grouped convolutions only remove connections from input channels to output channels while keeping the overall number of channels the same. As a result, channel pruning is the best performing compression structure when applied in isolation.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">In a next step, the group size is set to 64 and channel pruning is applied (<i>WRN: G=64 Channel </i>), in order to evaluate the performance when different sparse structures are combined. This combination performs worse than pure channel pruning for all metrics and requires a large amount of activations. Ultimately, it can be stated that group convolutions are excellent at reducing FLOPs and parameters but can harm the overall memory requirements by increasing the amount of activations.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">Last, the DenseNet variant is compressed using channel pruning (<i>Dense: Channel </i>). The dense architecture outperforms the residual blocks in terms of number of FLOPs as well as parameters. In terms of number of activations, however, residual blocks are clearly more beneficial, which also influences the overall memory consumption. In summary, one can observe that DenseNets are more parameter/computation-efficient and ResNets are more memory-efficient.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li data-list-text="5.2"><h3 style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><a name="bookmark69">Evaluating Compressed DNNs on Embedded Hardware</a></h3><ol id="l14"><li data-list-text="5.2.1"><p class="s59" style="padding-top: 6pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark70">Evaluating Compressed DNNs on CPU</a><a name="bookmark80">&zwnj;</a></p><p style="padding-top: 6pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark75" class="a">Section </a><a href="#bookmark81" class="a">5.1 explored the impact of several network quantization approaches and structured pruning on the prediction quality. In this section. we use the well-performing LQ-Net approach for quantization and PSP (for channel pruning) to measure the inference throughput of the quantized and pruned models separately on an ARM Cortex-A53 CPU. The WRN model on the CIFAR-10 task is used again as a baseline, with a depth of 28 layers, varying widths of the model, and weights/activations quantized to different bit widths. Figure </a>5 reports test accuracies and throughput for different WRN variants and compression methods. Please note that multiple points for the same bit width correspond to a different width scaling of the model.</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">The results reveal that quantization does not provide throughput improvements on this processor. This is mainly caused by the efficient floating-point units within the CPU in combination with fast on-chip memory and the high overhead caused by performing low-bit-width computations. Moreover, the dimensions of some layers are too small to fit well to the bit level vectorized instructions and these layers limit the overall performance. Together with the accuracy reduction, low-bit-width quantization does not yield convincing results on this processor. Quantized DNNs with 1-bit weights and activations are the worst</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="391" height="297" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a name="bookmark81">Figure 5: Throughput-accuracy trade-off of pruned and quantized WRN models on the CIFAR-10 task for an ARM CPU.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">performing models, which is due to the severe implications of extreme quantization on prediction performance. As can be seen, however, the overall performance of the quantized models increases considerably when the bit width of activations is increased to 2–3 bit whilst the bit width of the weights is kept low. On the contrary, channel pruning consistently performs equally to the baseline model with respect to accuracy and throughput. Pruning is therefore the more suitable compression technique for this embedded processor, especially when considering that CPUs could potentially leverage a much finer sparsity granularity.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="5.2.2"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark71">Evaluating Quantized DNNs on FPGAs</a><a name="bookmark82">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark187" class="a">While the previous section indicates that quantized DNNs do not provide throughput improvements on general-purpose processors without explicit hardware support, there are other hardware platforms where quantization is mandatory. Data-flow architectures, as found typically on FPGAs, where the main objective is to keep all required data for inference in on-chip memory, are usually constrained by the requirements for weight as well as activation storage. This section evaluates quantized DNNs on FPGAs using the FINN framework (Umuroglu et al., </a><a href="#bookmark83" class="a">2017) for generating data-flow architectures on reconfigurable hardware. Figure </a>6 shows test accuracy over throughput of the FINN data-flow architectures mapped to a XILINX Ultra96 FPGA using different bit combinations. A variant of the VGG architecture is used on the CIFAR-10 task for evaluation because FINN does not support residual connections yet, and the configuration of the FINN framework is adjusted so that highest throughput is targeted with respect to the available resources of the device (BRAM, LUTs, etc).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="391" height="297" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: left;"><a name="bookmark83">Figure 6: Throughput-accuracy trade-off of different quantized VGG models on the CIFAR-10 task for an FPGA data-flow architecture.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">As expected, the test accuracy increases gradually with high bit widths while the throughput decreases accordingly. Following the Pareto front starting from the bottom right indicates that the best performing models use a combination of 1 bit for the weights and a gradual increase of activations up to 3 bits. Afterwards the models perform best if the weights are scaled to 2 bits and the activation bit width is further increased to 4 bits. This supports the observation of the previous sections, showing that model accuracy is sensitive to activation quantization rather than weight quantization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="5.2.3"><p class="s59" style="padding-top: 1pt;padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark72">Evaluating Pruned DNNs on GPU</a><a name="bookmark84">&zwnj;</a></p><p style="padding-top: 7pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark78" class="a">Section 5.1.2 explored several pruning structures and DNN architectures which indicate different potential with respect to computation and memory efficiency. This section evaluates how these metrics impact the inference speed in terms of throughput. The same models are used as in Section </a><a href="#bookmark85" class="a">5.1.2, i.e., different WRN variants and DenseNet on the CIFAR-10 task. Figure </a>7 reports key inference metrics using the TensorRT framework targeting a Jetson Nano GPU. Half-precision floating-point precision is used for weights as well as activations and other reduced precision formats are omitted because the accelerator and its software infrastructure do not support arbitrary precision types.</p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">As can be seen, the various models in each regime (pruning structure or DNN architecture) show similar behavior for throughput. The worst performing regimes are group and kernel pruning as well as the combination of fixed grouping and channel pruning. Especially interesting is group pruning: although it greatly reduces FLOPs and parameters, it fails at translating this reduction into faster computations. Opposed to this, pure channel pruning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="391" height="305" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a name="bookmark85">Figure 7: Throughput-accuracy trade-off of different pruning methods for an embedded GPU on the CIFAR-10 task.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">using residual and dense architectures achieve the best performance. These results highlight the importance of reducing memory (or more specifically activations) rather than FLOPs in order to reduce latency or increase throughput.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li data-list-text="5.2.4"><p class="s59" style="padding-left: 37pt;text-indent: -31pt;text-align: left;"><a name="bookmark73">Overall Comparison</a><a name="bookmark86">&zwnj;</a></p><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a href="#bookmark87" class="a">The previous sections studied several compression methods with considerations on targeted software and hardware stack. Each section had a focus on leveraging compression to accelerate inference computations as much as possible while maintaining the prediction quality of the uncompressed model. In this section we compare these specialized forms of compression on their respective hardware in terms of absolute performance to identify the most promising compute concepts for DNNs. Notably, whilst fundamentally different in architecture, from a system-level view these three processors, namely ARM Cortex-A57 CPU, NVIDIA Nano GPU, and XILINX Ultra96 FPGA, are comparable as they all exhibit a power consumption in the range of about 5 Watts. Figure </a>8 reports throughput and accuracy for these devices using different compression methods.</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">CPUs are well suited for mapping compressed DNNs. However, the comparison to massively parallel processors shows that they lack the necessary amount of parallelism to achieve competitive throughput. The benefit of CPUs is that they feature a relatively large memory, allowing large and accurate models to be deployed.</p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">FPGAs excel at extremely-high-throughput inference and high utilization of the available hardware resources. Such data-flow architectures, however, demand the entire model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span><table border="0" cellspacing="0" cellpadding="0"><tr><td><img width="391" height="297" src="data:image/jpg;base64,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"/></td></tr></table></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><a name="bookmark87">Figure 8: Throughput-accuracy trade-off of different compression methods for different processor architectures (CPU, FPGA, GPU) on the CIFAR-10 task.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">(including activations) to stay on chip, possibly preventing larger and more accurate models to be deployed.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">GPUs are relatively constrained in terms of flexibility when deploying compressed models, due to the requirement of using optimized libraries and their respective software stack. However, they show a good compromise of programmable, general-purpose processing, enabling high throughput and accuracy. Additionally, they feature a large off-chip memory which in combination with latency hiding techniques enables high-throughput inference for large models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li></ol></li><li data-list-text="6."><h2 style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><a name="bookmark88">Conclusion</a></h2></li></ol><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">We presented an overview of the vast literature of the highly active research area concerned with resource efficiency of DNNs. We have identified three major directions of research, namely (i) network quantization, (ii) network pruning, and (iii) approaches that target efficiency at the structural level. Many of the presented works are orthogonal and can in principle be used in combination to potentially further improve the results reported in the respective papers.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">We have discovered several patterns in the individual strategies for enhancing resource efficiency. For quantization approaches, a common pattern in the most successful approaches is to combine real-valued representations, that help in maintaining the expressiveness of DNNs, with quantization to enhance the computationally intensive operations. More recently, mixed-precision quantization, where the bit widths are determined during training, is an upcoming topic. For pruning methods, we observed that the trend is moving towards</p><p style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">structured pruning approaches that obtain smaller models whose data structures are compatible with highly optimized dense tensor operations. On the structural level of DNNs, a lot of progress has been made in the development of specific architectures that maintain a high expressiveness of the DNN while at the same time reducing the computational overhead substantially. The newly emerging NAS approaches are promising candidates to automate the design of application-specific architectures with little user interaction. However, it appears unlikely that current NAS approaches will discover new fundamental design principles as the resulting architectures highly depend on a-priori knowledge encoded in the architecture search space.</p><p style="padding-left: 6pt;text-indent: 16pt;line-height: 108%;text-align: justify;">In experiments, we demonstrated the difficulty of finding a good trade-off among computational complexity, embedded hardware, and predictive performance on two benchmark datasets. We demonstrated on three embedded hardware platforms that massive parallelism is required for inference efficiency and that quantization as well as structured pruning map well onto these accelerators.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Acknowledgments</h2><p style="padding-top: 8pt;padding-left: 6pt;text-indent: 0pt;line-height: 108%;text-align: justify;">This work was supported by the Austrian Science Fund (FWF) under the project number I2706-N31 and the German Research Foundation (DFG) under the project number FR3273/1-1. Furthermore, we acknowledge the LEAD Project Dependable Internet of Things funded by Graz University of Technology. This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sk-lodowska-Curie Grant Agreement No. 797223 — HYBSPN. We acknowledge NVIDIA for providing GPU computing resources.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 6pt;text-indent: 0pt;text-align: left;">References</h2><p style="padding-top: 8pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark89">Jan Achterhold, Jan M. K¨ohler, Anke Schmeink, and Tim Genewein. Variational network quantization. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark90">Alexander G. Anderson and Cory P. Berg. The high-dimensional geometry of binary neural networks. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark91">Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2654–2662, 2014.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark92">Yoshua Bengio, Nicholas L´eonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. </a><i>CoRR</i>, abs/1308.3432, 2013.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark93">Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural networks. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 1613–1622, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark94">Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In </a><i>ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</i>, pages 535–541, 2006.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark95">Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark96">Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave Gaussian quantization. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 5406–5414, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark97">Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 2285–2294, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark98">Yu Cheng, Felix X. Yu, Rog´erio Schmidt Feris, Sanjiv Kumar, Alok N. Choudhary, and Shih-Fu Chang. An exploration of parameter redundancy in deep networks with circulant projections. In </a><i>International Conference on Computer Vision (ICCV)</i>, pages 2857–2865, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark99">Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. In </a><i>International Conference on Learning Representations (ICLR) Workshop</i>, volume abs/1412.7024, 2015a.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark100">Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural networks with binary weights during propagations. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 3123–3131, 2015b.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark101">Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predicting parameters in deep learning. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2148–2156, 2013.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark102">R. H. Dennard, F. H. Gaensslen, H. Yu, V. L. Rideout, E. Bassous, and A. R. LeBlanc. Design of ion-implanted MOSFET’s with very small physical dimensions. </a><i>IEEE Journal of Solid-State Circuits</i>, 1974.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark103">Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 1269–1277, 2014.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark104">Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated network with less inference complexity. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 1895–1903, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark105">Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ: Hessian aware quantization of neural networks with mixed-precision. In </a><i>International Conference on Computer Vision (ICCV)</i>, pages 293–302, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark106">Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2020.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark107">Xitong Gao, Yiren Zhao, Lukasz Dudziak, Robert Mullins, and Cheng-Zhong Xu. Dynamic channel pruning: Feature boosting and suppression. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark108">Alex Graves. Practical variational inference for neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2348–2356, 2011.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark109">Peter D. Gru¨nwald. </a><i>The minimum description length principle</i>. MIT press, 2007.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark110">Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 1379–1387, 2016.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark111">Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 1737–1746, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark112">Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 1135–1143, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark113">Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and Huffman coding. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark114">Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 164–171, 1992.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark115">Marton Havasi, Robert Peharz, and Jos´e Miguel Hern´andez-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark116">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 770–778, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark117">Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In </a><i>Conference on Computational Learning Theory (COLT)</i>, pages 5–13, 1993.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark118">Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In </a><i>Deep Learning and Representation Learning Workshop @ NIPS</i>, 2015.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark119">Markus H¨ohfeld and Scott E. Fahlman. Learning with limited numerical precision using the cascade-correlation algorithm. </a><i>IEEE Transactions on Neural Networks</i>, 3(4):602–611, 1992a.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark120">Markus H¨ohfeld and Scott E. Fahlman. Probabilistic rounding in neural network learning with limited precision. </a><i>Neurocomputing</i>, 4(6):291–299, 1992b.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark121">Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications. </a><i>CoRR</i>, abs/1704.04861, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark122">Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 2261–2269, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark123">Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In </a><i>European Conference on Computer Vision (ECCV)</i>, pages 317–334, 2018.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark124">Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 4107–4115, 2016.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark125">Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and</a></p><p class="s6" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">&lt;<span class="p">1mb model size. </span>CoRR<span class="p">, abs/1602.07360, 2016.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark126">Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 448–456, 2015.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark127">Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 2704–2713, 2018.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark128">Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In </a><i>British Machine Vision Conference (BMVC)</i>, 2014.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark129">Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-softmax. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark130">Jangho Kim, Seonguk Park, and Nojun Kwak. Paraphrasing complex network: Network compression via factor transfer. In </a><i>Advances in Neural Information Processing Systems (NeurIPS)</i>, pages 2765–2774, 2018.</p><p style="padding-top: 9pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark131">Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In</a></p><p class="s6" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">International Conference on Learning Representations (ICLR)<span class="p">, 2015.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark132">Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2575–2583, 2015.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark133">Anoop Korattikara, Vivek Rathod, Kevin P. Murphy, and Max Welling. Bayesian dark knowledge. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 3438–3446, 2015.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark134">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 1106–1114, 2012.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark135">Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, and Victor S. Lem-pitsky. Speeding-up convolutional neural networks using fine-tuned CP-decomposition. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark136">Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 598–605, 1989.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark137">Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. </a><i>CoRR</i>, abs/1605.04711, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark138">Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 5811–5821, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark139">Jinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan Gong. Learning small-size DNN with output-distribution-based criteria. In </a><i>INTERSPEECH: Conference of the International Speech Communication Association</i>, pages 1910–1914, 2014.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark140">Yang Li and Shihao Ji. </a><i>L</i><span class="s44">0</span>-ARM: Network sparsification via stochastic binary optimization. In <i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark141">Darryl Dexu Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 2849–2858, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark142">Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2181–2191, 2017a.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark143">Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2014a.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark144">Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ra-manan, Piotr Doll´ar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In </a><i>European Conference on Computer Vision (ECCV)</i>, pages 740–755, 2014b.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark145">Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In </a><i>Neural Information Processing Systems (NIPS)</i>, pages 345–353, 2017b.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark146">Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with few multiplications. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark147">Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019a.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark148">Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-Real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In </a><i>European Conference on Computer Vision (ECCV)</i>, pages 747–763, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark149">Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In </a><i>International Conference on Computer Vision (ICCV)</i>, pages 2755–2763, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark150">Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019b.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark151">Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 3288–3298, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark152">Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through L</a><span class="s44">0</span> regularization. In <i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark153">Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Relaxed quantization for discretized neural networks. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark154">Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep neural network compression. In </a><i>International Conference on Computer Vision (ICCV)</i>, pages 5068–5076, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark155">Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression using determinantal point processes. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark156">Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In </a><i>Uncertainty in Artificial Intelligence (UAI)</i>, pages 362–369, 2001.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark157">Asit K. Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark158">Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. </a><i>CoRR</i>, abs/1603.01025, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark159">Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep neural networks. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 2498–2507, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark160">Radford M. Neal. Bayesian training of backpropagation networks by the hybrid Monte Carlo method. Technical report, Dept. of Computer Science, University of Toronto, 1992.</a></p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark161">Jorge Nocedal and Stephen Wright. </a><i>Numerical Optimization</i>. Springer New York, 2 edition, 2006.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark162">Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry P. Vetrov. Tensorizing neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 442–450, 2015.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark163">Steven J. Nowlan and Geoffrey E. Hinton. Simplifying neural networks by soft weight-sharing. </a><i>Neural Computation</i>, 4(4):473–493, 1992.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark164">Jorn W. T. Peters and Max Welling.  Probabilistic binary neural networks.  </a><i>CoRR</i>, abs/1809.03368, 2018.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark165">Mary Phuong and Christoph Lampert. Distillation-based training for multi-exit architectures. In </a><i>IEEE International Conference on Computer Vision (ICCV)</i>, pages 1355–1364, 2019.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark166">Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark167">Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In </a><i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, pages 814–822, 2014.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark168">Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet classification using binary convolutional neural networks. In </a><i>European Conference on Computer Vision (ECCV)</i>, pages 525–542, 2016.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark169">Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for thin deep nets. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2015.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark170">Wolfgang Roth and Franz Pernkopf. Bayesian neural networks with weight sharing using Dirichlet processes. </a><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2018.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark171">Wolfgang Roth, Gu¨nther Schindler, Holger Fr¨oning, and Franz Pernkopf. Training discrete-valued neural networks with sign activations using weight distributions. In </a><i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</i>, 2019.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark172">David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. 323:533–536, 1986.</a></p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark173">Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 4510–4520, 2018.</p><p style="padding-top: 7pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark174">G. Schindler, W. Roth, F. Pernkopf, and H. Fr¨oning. Parameterized structured pruning for deep neural networks. In </a><i>6th International Conference on Machine Learning, Optimization, and Data Science (LOD)</i>, 2020.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark175">Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark176">Alexander Shekhovtsov, Viktor Yanush, and Boris Flach. Path sample-analytic gradient estimators for stochastic binary networks. In </a><i>Advances in Neural Information Processing Systems (NeurIPS)</i>, pages 12884–12894, 2020.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark177">Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark178">Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 963–971, 2014.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark179">Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. </a><i>Journal of Machine Learning Research</i>, 15(1):1929–1958, 2014.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark180">Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, and Diana Marculescu. Single-path NAS: Designing hardware-efficient convnets in less than 4 hours. In </a><i>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</i>, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark181">Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 1–9, 2015.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark182">Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 2818–2826, 2016.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark183">Mingxing Tan and Quoc V. Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 6105–6114, 2019.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark184">Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V. Le. MnasNet: Platform-aware neural architecture search for mobile. </a><i>CoRR</i>, abs/1807.11626, 2018.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark185">Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garc´ıa, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision DNNs: All you need is a good parametrization. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2020.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark186">Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2017.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark187">Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Philip Heng Wai Leong, Magnus Jahre, and Kees A. Vissers. FINN: A framework for fast, scalable binarized neural network inference. In </a><i>ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (ISFPGA)</i>, pages 65–74, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark188">Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. In </a><i>Advances in Neural Information Processing Systems (NeurIPS)</i>, pages 5741–5752, 2020.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark189">Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware automated quantization with mixed precision. In </a><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 8612–8620, 2019.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark190">Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In </a><i>International Conference on Machine Learning (ICML)</i>, pages 681–688, 2011.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark191">Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In </a><i>Advances in Neural Information Processing Systems (NIPS)</i>, pages 2074–2082, 2016.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark192">Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. 2018a.</a></p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark193">Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2018b.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark194">Saining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 5987–5995, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark195">Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alexander J. Smola, Le Song, and Ziyu Wang. Deep fried convnets. In </a><i>International Conference on Computer Vision (ICCV)</i>, pages 1476–1483, 2015.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark196">Mingzhang Yin and Mingyuan Zhou. ARM: augment-REINFORCE-merge gradient for stochastic binary networks. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2019.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark197">Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In </a><i>Proceedings of the British Machine Vision Conference (BMVC)</i>, 2016.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark198">Xinchuan Zeng and Tony R. Martinez. Using a neural network to approximate an ensemble of classifiers. </a><i>Neural Processing Letters</i>, 12(3):225–237, 2000.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark199">Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. LQ-Nets: Learned quantization for highly accurate and compact deep neural networks. In </a><i>European Conference on Computer Vision (ECCV)</i>, pages 373–390, 2018a.</p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark200">Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 6848–6856, 2018b.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark201">Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless CNNs with low-precision weights. In </a><i>International Conference on Learning Representations (ICLR)</i>, 2017.</p><p style="padding-top: 9pt;padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark202">Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients. </a><i>CoRR</i>, abs/1606.06160, 2016.</p><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark203">Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization.</a></p><p style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">In <i>International Conference on Learning Representations (ICLR)</i>, 2017.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark204">Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In</a></p><p class="s6" style="padding-top: 1pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">International Conference on Learning Representations (ICLR)<span class="p">, 2017.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: -10pt;line-height: 108%;text-align: justify;"><a name="bookmark205">Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. In </a><i>Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pages 8697–8710, 2018.</p></body></html>
